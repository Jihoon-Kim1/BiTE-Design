{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR5tnZUkw2ro"
      },
      "source": [
        "# BiTE Rigid Linker Analysis Pipeline\n",
        "## AI Co-Scientist Challenge Korea 2026\n",
        "\n",
        "### Pipeline Overview\n",
        "| Cell | Function | Output |\n",
        "|:----:|----------|--------|\n",
        "| 1 | Load RMSD functions | Functions ready |\n",
        "| 2 | Analyze structures & extract sequences | `sequence_df` |\n",
        "| 3 | Filter & create **combined** FASTA | `all_sequences.fasta` |\n",
        "| 4 | **AlphaFold (Fast Batch)** | PDB + JSON files |\n",
        "| 5 | **RMSD + Antigen Distance (130Å target)** | `comparison_df` |\n",
        "| 6 | Normal Mode Analysis (NMA) | `nma_results_df` |\n",
        "| 7 | pLDDT & PAE Analysis | `json_results_df` |\n",
        "| 8 | **Final Summary & Download** | Excel + ZIP |\n",
        "\n",
        "### ⚡ Key Features\n",
        "- **Fast MSA:** All sequences in one FASTA file (single MSA search)\n",
        "- **Antigen Distance:** Measures CD3e-HER2 distance after scFv alignment\n",
        "- **Target:** 130Å immunological synapse distance\n",
        "\n",
        "### Required Files\n",
        "Upload to `/content/`:\n",
        "- `reference.pdb` - Reference BiTE structure\n",
        "- `predictions/` - Folder with RFdiffusion PDB files\n",
        "- `Include_antigen_with_perfect_distance.pdb` - Antigen-antibody complex\n",
        "\n",
        "### Quick Start\n",
        "1. `Runtime → Change runtime type → GPU`\n",
        "2. Upload required files\n",
        "3. `Runtime → Run all`\n",
        "4. **First time:** Restart runtime when prompted, then `Run all` again\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2qmmb6Jw2rt"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 1: RMSD ANALYSIS FUNCTIONS\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 1: Load RMSD Analysis Functions** { display-mode: \"form\" }\n",
        "#@markdown This cell loads all core functions for RMSD calculation.\n",
        "#@markdown\n",
        "#@markdown **Run this cell first** - Functions will be reused in Cells 2 and 5.\n",
        "\n",
        "# --- Package Installation ---\n",
        "!pip install biopython --quiet\n",
        "\n",
        "# --- Imports ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from Bio.PDB import PDBParser, Superimposer, PPBuilder\n",
        "from Bio.PDB.Atom import Atom\n",
        "from Bio.PDB.Residue import Residue\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "\n",
        "# --- Constants ---\n",
        "SCFV1_LENGTH = 240  # First scFv domain length\n",
        "SCFV2_LENGTH = 242  # Second scFv domain length\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 1: Loading RMSD Analysis Functions\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# =============================================================================\n",
        "# CORE FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def get_ca_atoms(residues: List[Residue]) -> List[Atom]:\n",
        "    \"\"\"Extract CA (alpha carbon) atoms from a list of residues.\"\"\"\n",
        "    ca_atoms = []\n",
        "    for res in residues:\n",
        "        if 'CA' in res:\n",
        "            ca_atoms.append(res['CA'])\n",
        "    return ca_atoms\n",
        "\n",
        "\n",
        "def extract_residues(structure) -> List[Residue]:\n",
        "    \"\"\"Extract all amino acid residues from a structure.\"\"\"\n",
        "    residues = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if is_aa(residue, standard=True):\n",
        "                    residues.append(residue)\n",
        "    return residues\n",
        "\n",
        "\n",
        "def align_and_calculate_rmsd(\n",
        "    ref_atoms: List[Atom],\n",
        "    target_atoms: List[Atom],\n",
        "    full_target_atoms: List[Atom] = None\n",
        ") -> Tuple[float, List[Atom]]:\n",
        "    \"\"\"\n",
        "    Align target atoms to reference atoms and calculate RMSD.\n",
        "    Uses Superimposer to perform optimal superposition.\n",
        "    \"\"\"\n",
        "    if len(ref_atoms) != len(target_atoms):\n",
        "        raise ValueError(f\"Atom count mismatch: ref={len(ref_atoms)}, target={len(target_atoms)}\")\n",
        "\n",
        "    if len(ref_atoms) == 0:\n",
        "        raise ValueError(\"No atoms to align\")\n",
        "\n",
        "    sup = Superimposer()\n",
        "    sup.set_atoms(ref_atoms, target_atoms)\n",
        "\n",
        "    if full_target_atoms is not None:\n",
        "        sup.apply(full_target_atoms)\n",
        "    else:\n",
        "        sup.apply(target_atoms)\n",
        "\n",
        "    rmsd = sup.rms\n",
        "\n",
        "    return rmsd, target_atoms\n",
        "\n",
        "\n",
        "def calculate_rmsd_only(atoms1: List[Atom], atoms2: List[Atom]) -> float:\n",
        "    \"\"\"Calculate RMSD between two sets of atoms without alignment.\"\"\"\n",
        "    if len(atoms1) != len(atoms2):\n",
        "        raise ValueError(f\"Atom count mismatch: {len(atoms1)} vs {len(atoms2)}\")\n",
        "\n",
        "    coords1 = np.array([atom.get_coord() for atom in atoms1])\n",
        "    coords2 = np.array([atom.get_coord() for atom in atoms2])\n",
        "\n",
        "    diff = coords1 - coords2\n",
        "    rmsd = np.sqrt(np.mean(np.sum(diff**2, axis=1)))\n",
        "\n",
        "    return rmsd\n",
        "\n",
        "\n",
        "def load_reference_structure(ref_path: str) -> Tuple[List[Atom], List[Atom]]:\n",
        "    \"\"\"Load reference structure and extract scFv1 and scFv2 CA atoms.\"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    ref_structure = parser.get_structure('reference', ref_path)\n",
        "    ref_residues = extract_residues(ref_structure)\n",
        "\n",
        "    total_residues = len(ref_residues)\n",
        "    print(f\"Reference structure: {total_residues} residues\")\n",
        "\n",
        "    if total_residues < SCFV1_LENGTH + SCFV2_LENGTH:\n",
        "        raise ValueError(f\"Reference structure too short: {total_residues} residues\")\n",
        "\n",
        "    ref_scfv1 = ref_residues[:SCFV1_LENGTH]\n",
        "    ref_scfv2 = ref_residues[-SCFV2_LENGTH:]\n",
        "\n",
        "    ref_scfv1_ca = get_ca_atoms(ref_scfv1)\n",
        "    ref_scfv2_ca = get_ca_atoms(ref_scfv2)\n",
        "\n",
        "    print(f\"  • scFv1: {len(ref_scfv1_ca)} CA atoms\")\n",
        "    print(f\"  • scFv2: {len(ref_scfv2_ca)} CA atoms\")\n",
        "\n",
        "    return ref_scfv1_ca, ref_scfv2_ca\n",
        "\n",
        "\n",
        "def analyze_single_target(\n",
        "    ref_scfv1_ca: List[Atom],\n",
        "    ref_scfv2_ca: List[Atom],\n",
        "    target_path: str\n",
        ") -> Dict:\n",
        "    \"\"\"Analyze a single target structure against reference domains.\"\"\"\n",
        "    result = {\n",
        "        'Filename': Path(target_path).name,\n",
        "        'Status': 'Failed',\n",
        "        'Total_Residues': 0,\n",
        "        'Linker_Length': 0,\n",
        "        'scFv1_RMSD': None,\n",
        "        'scFv2_RMSD': None,\n",
        "        'Avg_RMSD': None,\n",
        "        'Final_RMSD': None,\n",
        "        'Best_Alignment': None,\n",
        "        'Error': None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        target_structure = parser.get_structure('target', target_path)\n",
        "        target_residues = extract_residues(target_structure)\n",
        "\n",
        "        total_residues = len(target_residues)\n",
        "        result['Total_Residues'] = total_residues\n",
        "\n",
        "        if total_residues < SCFV1_LENGTH + SCFV2_LENGTH:\n",
        "            result['Error'] = f\"Too few residues: {total_residues}\"\n",
        "            return result\n",
        "\n",
        "        linker_length = total_residues - SCFV1_LENGTH - SCFV2_LENGTH\n",
        "        result['Linker_Length'] = linker_length\n",
        "\n",
        "        target_scfv1 = target_residues[:SCFV1_LENGTH]\n",
        "        target_scfv2 = target_residues[-SCFV2_LENGTH:]\n",
        "\n",
        "        target_scfv1_ca = get_ca_atoms(target_scfv1)\n",
        "        target_scfv2_ca = get_ca_atoms(target_scfv2)\n",
        "        target_all_ca = get_ca_atoms(target_residues)\n",
        "\n",
        "        if len(target_scfv1_ca) != len(ref_scfv1_ca):\n",
        "            result['Error'] = f\"scFv1 atom mismatch: {len(target_scfv1_ca)} vs {len(ref_scfv1_ca)}\"\n",
        "            return result\n",
        "        if len(target_scfv2_ca) != len(ref_scfv2_ca):\n",
        "            result['Error'] = f\"scFv2 atom mismatch: {len(target_scfv2_ca)} vs {len(ref_scfv2_ca)}\"\n",
        "            return result\n",
        "\n",
        "        # Align on scFv1\n",
        "        rmsd_scfv1, _ = align_and_calculate_rmsd(\n",
        "            ref_scfv1_ca, target_scfv1_ca, target_all_ca\n",
        "        )\n",
        "\n",
        "        aligned_scfv2_ca = get_ca_atoms(target_residues[-SCFV2_LENGTH:])\n",
        "        rmsd_scfv2_after_scfv1_align = calculate_rmsd_only(ref_scfv2_ca, aligned_scfv2_ca)\n",
        "\n",
        "        parser2 = PDBParser(QUIET=True)\n",
        "        target_structure2 = parser2.get_structure('target2', target_path)\n",
        "        target_residues2 = extract_residues(target_structure2)\n",
        "        target_scfv2_ca_2 = get_ca_atoms(target_residues2[-SCFV2_LENGTH:])\n",
        "        target_all_ca_2 = get_ca_atoms(target_residues2)\n",
        "\n",
        "        # Align on scFv2\n",
        "        rmsd_scfv2, _ = align_and_calculate_rmsd(\n",
        "            ref_scfv2_ca, target_scfv2_ca_2, target_all_ca_2\n",
        "        )\n",
        "\n",
        "        aligned_scfv1_ca = get_ca_atoms(target_residues2[:SCFV1_LENGTH])\n",
        "        rmsd_scfv1_after_scfv2_align = calculate_rmsd_only(ref_scfv1_ca, aligned_scfv1_ca)\n",
        "\n",
        "        avg_rmsd_align_scfv1 = (rmsd_scfv1 + rmsd_scfv2_after_scfv1_align) / 2\n",
        "        avg_rmsd_align_scfv2 = (rmsd_scfv2 + rmsd_scfv1_after_scfv2_align) / 2\n",
        "\n",
        "        if avg_rmsd_align_scfv1 <= avg_rmsd_align_scfv2:\n",
        "            result['Final_RMSD'] = rmsd_scfv2_after_scfv1_align\n",
        "            result['Best_Alignment'] = 'scFv1'\n",
        "            result['scFv1_RMSD'] = rmsd_scfv1\n",
        "            result['scFv2_RMSD'] = rmsd_scfv2_after_scfv1_align\n",
        "        else:\n",
        "            result['Final_RMSD'] = rmsd_scfv1_after_scfv2_align\n",
        "            result['Best_Alignment'] = 'scFv2'\n",
        "            result['scFv1_RMSD'] = rmsd_scfv1_after_scfv2_align\n",
        "            result['scFv2_RMSD'] = rmsd_scfv2\n",
        "\n",
        "        result['Avg_RMSD'] = min(avg_rmsd_align_scfv1, avg_rmsd_align_scfv2)\n",
        "        result['Status'] = 'Success'\n",
        "\n",
        "    except Exception as e:\n",
        "        result['Error'] = str(e)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def analyze_batch(\n",
        "    ref_scfv1_ca: List[Atom],\n",
        "    ref_scfv2_ca: List[Atom],\n",
        "    target_folder: str,\n",
        "    pattern: str = \"*.pdb\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Analyze all PDB files in a folder.\"\"\"\n",
        "    target_path = Path(target_folder)\n",
        "    pdb_files = sorted(target_path.glob(pattern))\n",
        "\n",
        "    print(f\"\\nFound {len(pdb_files)} PDB files to analyze\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    results = []\n",
        "    for i, pdb_file in enumerate(pdb_files, 1):\n",
        "        print(f\"  [{i}/{len(pdb_files)}] {pdb_file.name}\", end=\"\")\n",
        "        result = analyze_single_target(ref_scfv1_ca, ref_scfv2_ca, str(pdb_file))\n",
        "\n",
        "        if result['Status'] == 'Success':\n",
        "            print(f\" → RMSD: {result['Final_RMSD']:.4f} Å ({result['Best_Alignment']})\")\n",
        "        else:\n",
        "            print(f\" → FAILED: {result['Error']}\")\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"✓ Functions loaded successfully!\")\n",
        "print(\"-\" * 70)\n",
        "print(\"\\nAvailable functions:\")\n",
        "print(\"  • load_reference_structure(ref_path)\")\n",
        "print(\"  • analyze_single_target(ref_scfv1_ca, ref_scfv2_ca, target_path)\")\n",
        "print(\"  • analyze_batch(ref_scfv1_ca, ref_scfv2_ca, target_folder, pattern)\")\n",
        "print(\"\\nDomain structure:\")\n",
        "print(f\"  • scFv1: {SCFV1_LENGTH} residues (N-terminal)\")\n",
        "print(f\"  • scFv2: {SCFV2_LENGTH} residues (C-terminal)\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeRvI6VRw2rv"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 2: INITIAL RMSD ANALYSIS & SEQUENCE EXTRACTION\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 2: Analyze Structures & Extract Sequences** { display-mode: \"form\" }\n",
        "#@markdown ### Configuration\n",
        "#@markdown Modify paths if needed:\n",
        "REFERENCE_PATH = \"/content/reference.pdb\"  #@param {type:\"string\"}\n",
        "TARGET_FOLDER = \"/content/predictions/\"  #@param {type:\"string\"}\n",
        "FILE_PATTERN = \"*.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Requires:** Cell 1 must be run first.\n",
        "\n",
        "# --- Display Settings ---\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 2: RMSD Analysis & Sequence Extraction\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# =============================================================================\n",
        "# SEQUENCE EXTRACTION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def extract_sequence_from_pdb(pdb_path: str) -> str:\n",
        "    \"\"\"Extract complete amino acid sequence from a PDB file.\"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('protein', pdb_path)\n",
        "    ppb = PPBuilder()\n",
        "\n",
        "    sequences = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for pp in ppb.build_peptides(chain):\n",
        "                sequences.append(str(pp.get_sequence()))\n",
        "\n",
        "    return ''.join(sequences)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  • Reference: {REFERENCE_PATH}\")\n",
        "print(f\"  • Target folder: {TARGET_FOLDER}\")\n",
        "print(f\"  • Pattern: {FILE_PATTERN}\")\n",
        "\n",
        "print(\"\\n--- Validation ---\")\n",
        "\n",
        "# Check paths\n",
        "if not Path(REFERENCE_PATH).exists():\n",
        "    raise FileNotFoundError(f\"Reference file not found: {REFERENCE_PATH}\")\n",
        "print(f\"✓ Reference file exists\")\n",
        "\n",
        "if not Path(TARGET_FOLDER).exists():\n",
        "    raise FileNotFoundError(f\"Target folder not found: {TARGET_FOLDER}\")\n",
        "\n",
        "pdb_count = len(list(Path(TARGET_FOLDER).glob(FILE_PATTERN)))\n",
        "if pdb_count == 0:\n",
        "    raise FileNotFoundError(f\"No PDB files found in {TARGET_FOLDER}\")\n",
        "print(f\"✓ Found {pdb_count} PDB files\")\n",
        "\n",
        "# =============================================================================\n",
        "# LOAD REFERENCE & ANALYZE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING REFERENCE STRUCTURE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "ref_scfv1, ref_scfv2 = load_reference_structure(REFERENCE_PATH)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYZING TARGET STRUCTURES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results_df = analyze_batch(ref_scfv1, ref_scfv2, TARGET_FOLDER, FILE_PATTERN)\n",
        "\n",
        "# =============================================================================\n",
        "# EXTRACT SEQUENCES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXTRACTING SEQUENCES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sequences = []\n",
        "for _, row in results_df.iterrows():\n",
        "    if row['Status'] == 'Success':\n",
        "        pdb_path = Path(TARGET_FOLDER) / row['Filename']\n",
        "        seq = extract_sequence_from_pdb(str(pdb_path))\n",
        "        sequences.append(seq)\n",
        "        print(f\"  ✓ {row['Filename']}: {len(seq)} aa\")\n",
        "    else:\n",
        "        sequences.append(None)\n",
        "        print(f\"  ✗ {row['Filename']}: Failed\")\n",
        "\n",
        "results_df['Sequence'] = sequences\n",
        "results_df['Sequence_Length'] = results_df['Sequence'].apply(lambda x: len(x) if x else 0)\n",
        "\n",
        "# =============================================================================\n",
        "# DETERMINE LINKER GROUP\n",
        "# =============================================================================\n",
        "\n",
        "def get_linker_group(linker_len):\n",
        "    \"\"\"Categorize linker length into groups.\"\"\"\n",
        "    if linker_len <= 35:\n",
        "        return \"Short (≤35)\"\n",
        "    elif linker_len <= 45:\n",
        "        return \"Medium (36-45)\"\n",
        "    elif linker_len <= 55:\n",
        "        return \"Long (46-55)\"\n",
        "    else:\n",
        "        return \"Very Long (>55)\"\n",
        "\n",
        "results_df['Linker_Group'] = results_df['Linker_Length'].apply(get_linker_group)\n",
        "\n",
        "# =============================================================================\n",
        "# DISPLAY RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYSIS RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Summary statistics\n",
        "successful = results_df[results_df['Status'] == 'Success']\n",
        "print(f\"\\nTotal files: {len(results_df)}\")\n",
        "print(f\"Successful: {len(successful)}\")\n",
        "print(f\"Failed: {len(results_df) - len(successful)}\")\n",
        "\n",
        "if len(successful) > 0:\n",
        "    print(f\"\\nRMSD Statistics:\")\n",
        "    print(f\"  • Mean: {successful['Final_RMSD'].mean():.4f} Å\")\n",
        "    print(f\"  • Std:  {successful['Final_RMSD'].std():.4f} Å\")\n",
        "    print(f\"  • Min:  {successful['Final_RMSD'].min():.4f} Å\")\n",
        "    print(f\"  • Max:  {successful['Final_RMSD'].max():.4f} Å\")\n",
        "\n",
        "# Display table\n",
        "print(\"\\n--- Results Table ---\")\n",
        "display_cols = ['Filename', 'Total_Residues', 'Linker_Length', 'Linker_Group',\n",
        "                'Final_RMSD', 'Best_Alignment', 'Sequence_Length']\n",
        "display_df = results_df[display_cols].copy()\n",
        "display_df['Final_RMSD'] = display_df['Final_RMSD'].apply(\n",
        "    lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\"\n",
        ")\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# =============================================================================\n",
        "# SAVE FOR NEXT CELL\n",
        "# =============================================================================\n",
        "\n",
        "sequence_df = results_df.copy()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 2 Complete\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nData saved to: sequence_df ({len(sequence_df)} rows)\")\n",
        "print(\"→ Run Cell 3 to filter and create FASTA files\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljjdnCijw2rw"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 3: FILTER BY RMSD & PREPARE COMBINED FASTA FILE\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 3: Filter & Create Combined FASTA** { display-mode: \"form\" }\n",
        "#@markdown ### Configuration\n",
        "RMSD_THRESHOLD = 30  #@param {type:\"number\"}\n",
        "FASTA_OUTPUT_DIR = \"/content/fasta_files/\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Note:** All sequences are combined into ONE FASTA file for faster MSA.\n",
        "#@markdown\n",
        "#@markdown **Requires:** Cell 2 must be run first.\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 3: Filter & Prepare Combined FASTA\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  • RMSD Threshold: ≤ {RMSD_THRESHOLD} Å\")\n",
        "print(f\"  • Output: {FASTA_OUTPUT_DIR}\")\n",
        "\n",
        "# =============================================================================\n",
        "# VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n--- Validation ---\")\n",
        "\n",
        "try:\n",
        "    _ = sequence_df\n",
        "    print(f\"✓ sequence_df found with {len(sequence_df)} sequences\")\n",
        "except NameError:\n",
        "    raise RuntimeError(\"✗ sequence_df not found! Run Cell 2 first.\")\n",
        "\n",
        "if len(sequence_df) == 0:\n",
        "    raise RuntimeError(\"✗ sequence_df is empty!\")\n",
        "\n",
        "# =============================================================================\n",
        "# FILTERING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FILTERING SEQUENCES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "filtered_df = sequence_df[\n",
        "    (sequence_df['Status'] == 'Success') &\n",
        "    (sequence_df['Final_RMSD'] <= RMSD_THRESHOLD) &\n",
        "    (sequence_df['Sequence'].notna()) &\n",
        "    (sequence_df['Sequence'].str.len() > 0)\n",
        "].copy()\n",
        "\n",
        "print(f\"\\nFiltering results:\")\n",
        "print(f\"  • Total sequences: {len(sequence_df)}\")\n",
        "print(f\"  • Successful: {len(sequence_df[sequence_df['Status'] == 'Success'])}\")\n",
        "print(f\"  • RMSD ≤ {RMSD_THRESHOLD}Å: {len(filtered_df)}\")\n",
        "\n",
        "if len(filtered_df) == 0:\n",
        "    print(\"\\n⚠️ No sequences passed the filter!\")\n",
        "    raise RuntimeError(\"No sequences to process\")\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE JOBNAMES\n",
        "# =============================================================================\n",
        "\n",
        "def clean_jobname(filename: str) -> str:\n",
        "    \"\"\"Create a clean jobname from filename.\"\"\"\n",
        "    name = filename.replace('.pdb', '')\n",
        "    name = re.sub(r'[^\\w\\-]', '_', name)\n",
        "    name = re.sub(r'_+', '_', name)\n",
        "    name = name.strip('_')\n",
        "    return name\n",
        "\n",
        "def format_jobname(filename: str) -> str:\n",
        "    return clean_jobname(filename)\n",
        "\n",
        "jobnames = []\n",
        "for _, row in filtered_df.iterrows():\n",
        "    jobnames.append(format_jobname(row['Filename']))\n",
        "\n",
        "filtered_df['Jobname'] = jobnames\n",
        "filtered_df['RMSD'] = filtered_df['Final_RMSD']\n",
        "\n",
        "# =============================================================================\n",
        "# CREATE COMBINED FASTA FILE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING COMBINED FASTA FILE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "os.makedirs(FASTA_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Combined FASTA path\n",
        "COMBINED_FASTA_PATH = os.path.join(FASTA_OUTPUT_DIR, \"all_sequences.fasta\")\n",
        "\n",
        "print(f\"\\nCombining {len(filtered_df)} sequences into one FASTA file...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "with open(COMBINED_FASTA_PATH, 'w') as f:\n",
        "    for idx, row in filtered_df.iterrows():\n",
        "        jobname = row['Jobname']\n",
        "        sequence = row['Sequence']\n",
        "\n",
        "        # Write FASTA entry\n",
        "        f.write(f\">{jobname}\\n\")\n",
        "        # Write sequence in lines of 80 characters\n",
        "        for i in range(0, len(sequence), 80):\n",
        "            f.write(f\"{sequence[i:i+80]}\\n\")\n",
        "\n",
        "        print(f\"  ✓ {jobname}: {len(sequence)} aa\")\n",
        "\n",
        "print(f\"\\n✓ Combined FASTA saved: {COMBINED_FASTA_PATH}\")\n",
        "\n",
        "# Store path for Cell 4\n",
        "filtered_df['FASTA_Path'] = COMBINED_FASTA_PATH\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n✓ {len(filtered_df)} sequences combined into: {COMBINED_FASTA_PATH}\")\n",
        "print(f\"\\n⚡ MSA will be computed ONCE for all sequences (much faster!)\")\n",
        "\n",
        "# Statistics by linker group\n",
        "if 'Linker_Group' in filtered_df.columns:\n",
        "    print(\"\\n--- By Linker Group ---\")\n",
        "    group_stats = filtered_df.groupby('Linker_Group').agg({\n",
        "        'Jobname': 'count',\n",
        "        'Final_RMSD': ['mean', 'std']\n",
        "    }).round(4)\n",
        "    group_stats.columns = ['Count', 'Mean_RMSD', 'Std_RMSD']\n",
        "    print(group_stats.to_string())\n",
        "\n",
        "# Display filtered sequences\n",
        "print(\"\\n--- Filtered Sequences ---\")\n",
        "display_cols = ['Jobname', 'Linker_Length', 'Linker_Group', 'Final_RMSD', 'Sequence_Length']\n",
        "display_df = filtered_df[display_cols].copy()\n",
        "display_df['Final_RMSD'] = display_df['Final_RMSD'].apply(lambda x: f\"{x:.4f}\")\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Create fasta_df for Cell 4\n",
        "fasta_df = filtered_df[['Jobname', 'Filename', 'Sequence', 'Sequence_Length',\n",
        "                        'Linker_Length', 'Linker_Group', 'RMSD']].copy()\n",
        "fasta_df['Original_Filename'] = filtered_df['Filename']\n",
        "\n",
        "# Store combined FASTA path as global variable\n",
        "COMBINED_FASTA = COMBINED_FASTA_PATH\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 3 Complete\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nData saved to:\")\n",
        "print(f\"  • filtered_df: {len(filtered_df)} sequences\")\n",
        "print(f\"  • fasta_df: {len(fasta_df)} sequences\")\n",
        "print(f\"  • COMBINED_FASTA: {COMBINED_FASTA}\")\n",
        "print(f\"\\n→ Run Cell 4 to start AlphaFold predictions\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzee-4Raw2rw"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 4: ALPHAFOLD BATCH EXECUTION (COMBINED FASTA - FAST MSA)\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 4: AlphaFold Predictions (Fast Batch)** { display-mode: \"form\" }\n",
        "#@markdown ### AlphaFold Configuration\n",
        "NUM_MODELS = 1  #@param {type:\"integer\"}\n",
        "NUM_RECYCLES = 3  #@param {type:\"integer\"}\n",
        "USE_AMBER = False  #@param {type:\"boolean\"}\n",
        "MSA_MODE = \"mmseqs2_uniref_env\"  #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\", \"single_sequence\"]\n",
        "TIMEOUT_MINUTES = 1000  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **⚡ Fast Mode:** All sequences use ONE MSA search (saves ~5min per sequence)\n",
        "#@markdown\n",
        "#@markdown **First run:** ~15 min installation + runtime restart required.\n",
        "\n",
        "# CRITICAL: Set backend before any matplotlib import\n",
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "# Paths\n",
        "ALPHAFOLD_OUTPUT_DIR = \"/content/alphafold_results/\"\n",
        "FASTA_INPUT_DIR = \"/content/fasta_files/\"\n",
        "CONDA_PATH = \"/opt/miniforge\"\n",
        "CONDA_ENV = f\"{CONDA_PATH}/envs/colabfold\"\n",
        "COLABFOLD_BIN = f\"{CONDA_ENV}/bin/colabfold_batch\"\n",
        "MAMBA_BIN = f\"{CONDA_PATH}/bin/mamba\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 4: AlphaFold Batch Execution (Fast MSA Mode)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  • Models: {NUM_MODELS}\")\n",
        "print(f\"  • Recycles: {NUM_RECYCLES}\")\n",
        "print(f\"  • AMBER: {USE_AMBER}\")\n",
        "print(f\"  • MSA mode: {MSA_MODE}\")\n",
        "print(f\"  • Timeout: {TIMEOUT_MINUTES} min (total)\")\n",
        "print(f\"\\n⚡ Fast Mode: Single MSA for all sequences!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: GPU CHECK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 1/4] Checking GPU...\")\n",
        "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    gpu_name = result.stdout.strip().split('\\n')[0]\n",
        "    print(f\"  ✓ {gpu_name}\")\n",
        "else:\n",
        "    raise RuntimeError(\"GPU required - Runtime → Change runtime type → GPU\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: CHECK/INSTALL COLABFOLD\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 2/4] Setting up ColabFold environment...\")\n",
        "\n",
        "def check_installation_complete():\n",
        "    return all([\n",
        "        os.path.exists(COLABFOLD_BIN),\n",
        "        os.path.exists(f\"{CONDA_ENV}/bin/python\"),\n",
        "        os.path.exists(MAMBA_BIN),\n",
        "    ])\n",
        "\n",
        "def test_colabfold_runs():\n",
        "    if not os.path.exists(COLABFOLD_BIN):\n",
        "        return False\n",
        "    try:\n",
        "        r = subprocess.run([COLABFOLD_BIN, '--help'],\n",
        "                          capture_output=True, text=True, timeout=30)\n",
        "        return r.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if check_installation_complete() and test_colabfold_runs():\n",
        "    print(\"  ✓ ColabFold is ready!\")\n",
        "else:\n",
        "    print(\"  Installing ColabFold...\")\n",
        "    print(\"  This takes 12-18 minutes (one-time setup)\\n\")\n",
        "\n",
        "    # Install Miniforge\n",
        "    print(\"  [1/4] Installing Miniforge...\")\n",
        "    if not os.path.exists(MAMBA_BIN):\n",
        "        miniforge_url = \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\"\n",
        "        cmd = f'''\n",
        "wget -q --timeout=120 \"{miniforge_url}\" -O /tmp/miniforge.sh || curl -sL --max-time 120 \"{miniforge_url}\" -o /tmp/miniforge.sh\n",
        "bash /tmp/miniforge.sh -b -p {CONDA_PATH} -u\n",
        "rm -f /tmp/miniforge.sh\n",
        "'''\n",
        "        subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n",
        "\n",
        "    if os.path.exists(MAMBA_BIN):\n",
        "        print(\"  ✓ Miniforge ready\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Miniforge installation failed\")\n",
        "\n",
        "    # Create environment\n",
        "    print(\"  [2/4] Creating Python 3.10 environment...\")\n",
        "    if not os.path.exists(f\"{CONDA_ENV}/bin/python\"):\n",
        "        cmd = f'{MAMBA_BIN} create -y -p {CONDA_ENV} python=3.10 -q'\n",
        "        subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "    if os.path.exists(f\"{CONDA_ENV}/bin/python\"):\n",
        "        print(\"  ✓ Environment created\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Environment creation failed\")\n",
        "\n",
        "    # Install pdbfixer + openmm\n",
        "    print(\"  [3/4] Installing pdbfixer + OpenMM...\")\n",
        "    cmd = f'{MAMBA_BIN} install -y -p {CONDA_ENV} -c conda-forge pdbfixer openmm -q'\n",
        "    subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n",
        "    print(\"  ✓ pdbfixer + OpenMM installed\")\n",
        "\n",
        "    # Install ColabFold + JAX\n",
        "    print(\"  [4/4] Installing ColabFold + JAX...\")\n",
        "    cmd1 = f'{CONDA_ENV}/bin/pip install -q \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"'\n",
        "    subprocess.run(cmd1, shell=True, capture_output=True, text=True, timeout=600)\n",
        "\n",
        "    cmd2 = f'{CONDA_ENV}/bin/pip install -q \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html'\n",
        "    subprocess.run(cmd2, shell=True, capture_output=True, text=True, timeout=300)\n",
        "\n",
        "    if os.path.exists(COLABFOLD_BIN):\n",
        "        print(\"  ✓ ColabFold + JAX installed\")\n",
        "    else:\n",
        "        raise RuntimeError(\"ColabFold installation failed\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ INSTALLATION COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\"\"\n",
        "⚠️  RUNTIME RESTART REQUIRED\n",
        "\n",
        "Please:\n",
        "  1. Click: Runtime → Restart runtime\n",
        "  2. Wait ~10 seconds\n",
        "  3. Click: Runtime → Run all\n",
        "\n",
        "After restart, predictions will run automatically.\n",
        "\"\"\")\n",
        "    print(\"=\" * 70)\n",
        "    sys.exit(0)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: CHECK PREREQUISITES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 3/4] Checking prerequisites...\")\n",
        "\n",
        "# Check fasta_df\n",
        "try:\n",
        "    _ = fasta_df\n",
        "    print(f\"  ✓ fasta_df: {len(fasta_df)} sequences\")\n",
        "except NameError:\n",
        "    raise RuntimeError(\"fasta_df not found. Run Cells 1-3 first.\")\n",
        "\n",
        "# Check combined FASTA file\n",
        "try:\n",
        "    _ = COMBINED_FASTA\n",
        "    if os.path.exists(COMBINED_FASTA):\n",
        "        print(f\"  ✓ Combined FASTA: {COMBINED_FASTA}\")\n",
        "    else:\n",
        "        raise FileNotFoundError()\n",
        "except:\n",
        "    # Fallback: look for all_sequences.fasta\n",
        "    COMBINED_FASTA = os.path.join(FASTA_INPUT_DIR, \"all_sequences.fasta\")\n",
        "    if os.path.exists(COMBINED_FASTA):\n",
        "        print(f\"  ✓ Combined FASTA found: {COMBINED_FASTA}\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Combined FASTA not found. Run Cell 3 first.\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(ALPHAFOLD_OUTPUT_DIR)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"  ✓ Output: {output_dir}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: RUN ALPHAFOLD (SINGLE BATCH)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 4/4] Running AlphaFold predictions...\")\n",
        "print(f\"\\n  ⚡ Processing {len(fasta_df)} sequences with SINGLE MSA search\")\n",
        "print(f\"  Estimated time: ~5min MSA + ~{len(fasta_df) * 2}-{len(fasta_df) * 3}min predictions\")\n",
        "\n",
        "# Temporary output directory for ColabFold\n",
        "temp_output_dir = output_dir / \"temp_batch\"\n",
        "temp_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Build command\n",
        "cmd = [COLABFOLD_BIN]\n",
        "cmd.extend([\"--num-recycle\", str(NUM_RECYCLES)])\n",
        "cmd.extend([\"--num-models\", str(NUM_MODELS)])\n",
        "cmd.extend([\"--model-type\", \"alphafold2_ptm\"])\n",
        "cmd.extend([\"--msa-mode\", MSA_MODE])\n",
        "cmd.extend([\"--rank\", \"plddt\"])\n",
        "\n",
        "if USE_AMBER:\n",
        "    cmd.extend([\"--amber\", \"--use-gpu-relax\"])\n",
        "\n",
        "cmd.extend([COMBINED_FASTA, str(temp_output_dir)])\n",
        "\n",
        "print(f\"\\n  Command: colabfold_batch [combined.fasta] [output_dir]\")\n",
        "print(f\"  Running...\\n\")\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=TIMEOUT_MINUTES * 60\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - t_start\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"  ⚠️ ColabFold returned non-zero exit code\")\n",
        "        out = (result.stdout or \"\") + (result.stderr or \"\")\n",
        "        if \"pdbfixer\" in out.lower():\n",
        "            print(\"    → Restart runtime, re-run all cells\")\n",
        "        elif \"mmseqs\" in out.lower():\n",
        "            print(\"    → MSA server issue, try again or use 'single_sequence'\")\n",
        "        else:\n",
        "            lines = [l for l in out.strip().split('\\n') if l.strip()][-5:]\n",
        "            for line in lines:\n",
        "                print(f\"    {line[:80]}\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(f\"  ✗ Timeout after {TIMEOUT_MINUTES} minutes\")\n",
        "    subprocess.run('pkill -9 -f colabfold', shell=True, capture_output=True)\n",
        "    raise RuntimeError(\"Prediction timed out\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ✗ Exception: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\n  Total batch time: {elapsed/60:.1f} minutes\")\n",
        "\n",
        "# ============================================================================\n",
        "# ORGANIZE OUTPUT FILES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ORGANIZING OUTPUT FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "successful = []\n",
        "failed = []\n",
        "\n",
        "# Find all generated PDB files\n",
        "if USE_AMBER:\n",
        "    pdb_pattern = \"*_relaxed_rank_001*.pdb\"\n",
        "else:\n",
        "    pdb_pattern = \"*_unrelaxed_rank_001*.pdb\"\n",
        "\n",
        "generated_pdbs = list(temp_output_dir.glob(pdb_pattern))\n",
        "if not generated_pdbs:\n",
        "    generated_pdbs = list(temp_output_dir.glob(\"*rank_001*.pdb\"))\n",
        "if not generated_pdbs:\n",
        "    generated_pdbs = list(temp_output_dir.glob(\"*.pdb\"))\n",
        "\n",
        "print(f\"\\nFound {len(generated_pdbs)} PDB files\")\n",
        "\n",
        "# Map generated files to jobnames\n",
        "for _, row in fasta_df.iterrows():\n",
        "    jobname = row['Jobname']\n",
        "\n",
        "    # Find matching PDB (ColabFold uses the FASTA header as prefix)\n",
        "    matching_pdbs = [p for p in generated_pdbs if jobname in p.name]\n",
        "\n",
        "    if matching_pdbs:\n",
        "        src_pdb = matching_pdbs[0]\n",
        "\n",
        "        # Determine type\n",
        "        if \"relaxed\" in src_pdb.name:\n",
        "            pdb_type = \"relaxed\"\n",
        "        else:\n",
        "            pdb_type = \"unrelaxed\"\n",
        "\n",
        "        # Copy to final location with clean name\n",
        "        final_pdb = output_dir / f\"{jobname}_{pdb_type}_rank_001.pdb\"\n",
        "        shutil.copy(src_pdb, final_pdb)\n",
        "\n",
        "        # Find matching JSON\n",
        "        json_pattern = jobname + \"*scores*.json\"\n",
        "        matching_jsons = list(temp_output_dir.glob(json_pattern))\n",
        "        if not matching_jsons:\n",
        "            matching_jsons = list(temp_output_dir.glob(f\"{jobname}*.json\"))\n",
        "\n",
        "        final_json = None\n",
        "        if matching_jsons:\n",
        "            final_json = output_dir / f\"{jobname}_scores_rank_001.json\"\n",
        "            shutil.copy(matching_jsons[0], final_json)\n",
        "\n",
        "        successful.append({\n",
        "            'Jobname': jobname,\n",
        "            'Original_Filename': row['Original_Filename'],\n",
        "            'Original_RMSD': row['RMSD'],\n",
        "            'Linker_Group': row['Linker_Group'],\n",
        "            'Sequence_Length': row['Sequence_Length'],\n",
        "            'AlphaFold_PDB': str(final_pdb),\n",
        "            'AlphaFold_JSON': str(final_json) if final_json else None,\n",
        "            'PDB_Type': pdb_type,\n",
        "            'Status': 'Success'\n",
        "        })\n",
        "\n",
        "        j = \"✓\" if final_json else \"✗\"\n",
        "        print(f\"  ✓ {jobname}: PDB ✓ | JSON {j}\")\n",
        "    else:\n",
        "        failed.append({'Jobname': jobname, 'Error': 'PDB not found'})\n",
        "        print(f\"  ✗ {jobname}: PDB not found\")\n",
        "\n",
        "# Cleanup temp directory\n",
        "shutil.rmtree(temp_output_dir, ignore_errors=True)\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXECUTION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nTotal time: {elapsed/60:.1f} minutes\")\n",
        "print(f\"Successful: {len(successful)}/{len(fasta_df)}\")\n",
        "\n",
        "if len(successful) > 0:\n",
        "    avg_time = elapsed / len(successful)\n",
        "    print(f\"Average: {avg_time/60:.1f} min per sequence (including shared MSA)\")\n",
        "\n",
        "if failed:\n",
        "    print(\"\\n✗ Failed:\")\n",
        "    for f in failed:\n",
        "        print(f\"  • {f['Jobname']}: {f['Error']}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "if successful:\n",
        "    alphafold_results_df = pd.DataFrame(successful)\n",
        "    alphafold_results_df.to_csv(output_dir / \"alphafold_results.csv\", index=False)\n",
        "    print(f\"\\n✓ Results saved to {output_dir}\")\n",
        "else:\n",
        "    alphafold_results_df = pd.DataFrame()\n",
        "    print(\"\\n⚠️ No successful predictions\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 4 Complete\")\n",
        "print(\"=\" * 70)\n",
        "if successful:\n",
        "    print(\"→ Run Cell 5 for RMSD analysis\")\n",
        "    print(\"→ Run Cell 6 for NMA analysis\")\n",
        "    print(\"→ Run Cell 7 for pLDDT/PAE analysis\")\n",
        "    print(\"→ Run Cell 8 for Final Summary & Download\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh5uvPROw2rx"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 5: ALPHAFOLD RESULTS ANALYSIS & ANTIGEN DISTANCE MEASUREMENT\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 5: RMSD Comparison & Antigen Distance (130Å Target)** { display-mode: \"form\" }\n",
        "#@markdown ### Configuration\n",
        "REFERENCE_PATH = \"/content/reference.pdb\"  #@param {type:\"string\"}\n",
        "ANTIGEN_COMPLEX_PATH = \"/content/Include_antigen_with_perfect_distance.pdb\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Antigen Distance Settings\n",
        "#@markdown Chain B (CD3e) membrane-proximal residue:\n",
        "CHAIN_B_RESNUM = 102  #@param {type:\"integer\"}\n",
        "#@markdown Chain D (HER2) membrane-proximal residue:\n",
        "CHAIN_D_RESNUM = 652  #@param {type:\"integer\"}\n",
        "#@markdown Target immunological synapse distance:\n",
        "TARGET_DISTANCE = 130.0  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Requires:** Cells 1-4 must be run first.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 5: AlphaFold Analysis & Antigen Distance Measurement\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "ALPHAFOLD_OUTPUT_DIR = \"/content/alphafold_results/\"\n",
        "\n",
        "# =============================================================================\n",
        "# VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n--- Validation ---\")\n",
        "\n",
        "# Check alphafold_results_df\n",
        "try:\n",
        "    _ = alphafold_results_df\n",
        "    if len(alphafold_results_df) == 0:\n",
        "        raise ValueError(\"alphafold_results_df is empty\")\n",
        "    print(f\"✓ alphafold_results_df: {len(alphafold_results_df)} predictions\")\n",
        "except (NameError, ValueError) as e:\n",
        "    print(f\"⚠️ alphafold_results_df issue: {e}\")\n",
        "    print(\"  Attempting to find AlphaFold outputs directly...\")\n",
        "\n",
        "    af_output_path = Path(ALPHAFOLD_OUTPUT_DIR)\n",
        "    if af_output_path.exists():\n",
        "        af_pdbs = list(af_output_path.glob(\"*_relaxed_rank_001.pdb\"))\n",
        "        if not af_pdbs:\n",
        "            af_pdbs = list(af_output_path.glob(\"*_unrelaxed_rank_001.pdb\"))\n",
        "        if not af_pdbs:\n",
        "            af_pdbs = list(af_output_path.glob(\"*rank_001*.pdb\"))\n",
        "        if af_pdbs:\n",
        "            print(f\"  Found {len(af_pdbs)} AlphaFold PDB files\")\n",
        "            alphafold_results_df = pd.DataFrame([{\n",
        "                'Jobname': pdb.stem.replace('_relaxed_rank_001', '').replace('_unrelaxed_rank_001', ''),\n",
        "                'AlphaFold_PDB': str(pdb),\n",
        "                'Status': 'Success'\n",
        "            } for pdb in af_pdbs])\n",
        "        else:\n",
        "            raise RuntimeError(\"No AlphaFold output files found. Run Cell 4 first.\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"Output directory not found: {ALPHAFOLD_OUTPUT_DIR}\")\n",
        "\n",
        "# Check filtered_df\n",
        "try:\n",
        "    _ = filtered_df\n",
        "    print(f\"✓ filtered_df: {len(filtered_df)} sequences\")\n",
        "except NameError:\n",
        "    print(\"⚠️ filtered_df not found - original RMSD values may not be available\")\n",
        "    filtered_df = None\n",
        "\n",
        "# Check reference file\n",
        "if not Path(REFERENCE_PATH).exists():\n",
        "    raise FileNotFoundError(f\"Reference file not found: {REFERENCE_PATH}\")\n",
        "print(f\"✓ Reference file found\")\n",
        "\n",
        "# Check antigen complex file\n",
        "if not Path(ANTIGEN_COMPLEX_PATH).exists():\n",
        "    print(f\"⚠️ Antigen complex not found: {ANTIGEN_COMPLEX_PATH}\")\n",
        "    print(\"  Antigen distance measurement will be skipped.\")\n",
        "    ANTIGEN_COMPLEX_AVAILABLE = False\n",
        "else:\n",
        "    print(f\"✓ Antigen complex found\")\n",
        "    ANTIGEN_COMPLEX_AVAILABLE = True\n",
        "\n",
        "# =============================================================================\n",
        "# LOAD ANTIGEN-ANTIBODY COMPLEX\n",
        "# =============================================================================\n",
        "\n",
        "if ANTIGEN_COMPLEX_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LOADING ANTIGEN-ANTIBODY COMPLEX\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    antigen_complex = parser.get_structure('antigen_complex', ANTIGEN_COMPLEX_PATH)\n",
        "\n",
        "    # Extract chains\n",
        "    chain_A_residues = []  # scFv1 (anti-CD3)\n",
        "    chain_B_residues = []  # CD3e antigen\n",
        "    chain_C_residues = []  # scFv2 (anti-HER2)\n",
        "    chain_D_residues = []  # HER2 antigen\n",
        "\n",
        "    for model in antigen_complex:\n",
        "        for chain in model:\n",
        "            chain_id = chain.get_id()\n",
        "            for residue in chain:\n",
        "                if is_aa(residue, standard=True):\n",
        "                    if chain_id == 'A':\n",
        "                        chain_A_residues.append(residue)\n",
        "                    elif chain_id == 'B':\n",
        "                        chain_B_residues.append(residue)\n",
        "                    elif chain_id == 'C':\n",
        "                        chain_C_residues.append(residue)\n",
        "                    elif chain_id == 'D':\n",
        "                        chain_D_residues.append(residue)\n",
        "\n",
        "    print(f\"  Chain A (scFv1): {len(chain_A_residues)} residues\")\n",
        "    print(f\"  Chain B (CD3e): {len(chain_B_residues)} residues\")\n",
        "    print(f\"  Chain C (scFv2): {len(chain_C_residues)} residues\")\n",
        "    print(f\"  Chain D (HER2): {len(chain_D_residues)} residues\")\n",
        "\n",
        "    # Get CA atoms for alignment\n",
        "    chain_A_ca = get_ca_atoms(chain_A_residues)\n",
        "    chain_C_ca = get_ca_atoms(chain_C_residues)\n",
        "\n",
        "    # Find target residues for distance measurement\n",
        "    chain_B_target_ca = None\n",
        "    chain_D_target_ca = None\n",
        "\n",
        "    for model in antigen_complex:\n",
        "        if 'B' in [c.get_id() for c in model]:\n",
        "            chain_B = model['B']\n",
        "            for residue in chain_B:\n",
        "                if residue.get_id()[1] == CHAIN_B_RESNUM and 'CA' in residue:\n",
        "                    chain_B_target_ca = residue['CA']\n",
        "                    break\n",
        "\n",
        "        if 'D' in [c.get_id() for c in model]:\n",
        "            chain_D = model['D']\n",
        "            for residue in chain_D:\n",
        "                if residue.get_id()[1] == CHAIN_D_RESNUM and 'CA' in residue:\n",
        "                    chain_D_target_ca = residue['CA']\n",
        "                    break\n",
        "\n",
        "    if chain_B_target_ca and chain_D_target_ca:\n",
        "        original_distance = chain_B_target_ca - chain_D_target_ca\n",
        "        print(f\"\\n  Target residues found:\")\n",
        "        print(f\"    Chain B res {CHAIN_B_RESNUM} CA: {chain_B_target_ca.get_coord()}\")\n",
        "        print(f\"    Chain D res {CHAIN_D_RESNUM} CA: {chain_D_target_ca.get_coord()}\")\n",
        "        print(f\"    Original distance: {original_distance:.2f} Å\")\n",
        "        print(f\"    Target distance: {TARGET_DISTANCE:.2f} Å\")\n",
        "    else:\n",
        "        print(f\"  ⚠️ Could not find target residues for distance measurement\")\n",
        "        ANTIGEN_COMPLEX_AVAILABLE = False\n",
        "\n",
        "# =============================================================================\n",
        "# ANTIGEN DISTANCE CALCULATION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_antigen_distance(af_pdb_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Calculate the distance between antigen membrane-proximal residues\n",
        "    after superimposing the antigen-scFv complexes onto the AlphaFold structure.\n",
        "\n",
        "    Method:\n",
        "    1. Superimpose Chain A (scFv1) onto AlphaFold's scFv1 → get transformation matrix\n",
        "    2. Apply the same transformation to Chain B (CD3e) → CD3e moves with scFv1\n",
        "    3. Superimpose Chain C (scFv2) onto AlphaFold's scFv2 → get transformation matrix\n",
        "    4. Apply the same transformation to Chain D (HER2) → HER2 moves with scFv2\n",
        "    5. Measure distance between transformed Chain B res 102 and Chain D res 652\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'Antigen_Distance': None,\n",
        "        'Distance_From_Target': None,\n",
        "        'scFv1_Alignment_RMSD': None,\n",
        "        'scFv2_Alignment_RMSD': None\n",
        "    }\n",
        "\n",
        "    if not ANTIGEN_COMPLEX_AVAILABLE:\n",
        "        return result\n",
        "\n",
        "    try:\n",
        "        # Load AlphaFold structure\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        af_structure = parser.get_structure('alphafold', af_pdb_path)\n",
        "        af_residues = extract_residues(af_structure)\n",
        "\n",
        "        # Extract AlphaFold scFv regions\n",
        "        af_scfv1_residues = af_residues[:SCFV1_LENGTH]\n",
        "        af_scfv2_residues = af_residues[-SCFV2_LENGTH:]\n",
        "\n",
        "        af_scfv1_ca = get_ca_atoms(af_scfv1_residues)\n",
        "        af_scfv2_ca = get_ca_atoms(af_scfv2_residues)\n",
        "\n",
        "        # Check atom counts match\n",
        "        if len(af_scfv1_ca) != len(chain_A_ca):\n",
        "            result['Error'] = f\"scFv1 atom mismatch: AF={len(af_scfv1_ca)}, ChainA={len(chain_A_ca)}\"\n",
        "            return result\n",
        "        if len(af_scfv2_ca) != len(chain_C_ca):\n",
        "            result['Error'] = f\"scFv2 atom mismatch: AF={len(af_scfv2_ca)}, ChainC={len(chain_C_ca)}\"\n",
        "            return result\n",
        "\n",
        "        # === Superimpose Chain A onto AlphaFold scFv1 ===\n",
        "        # We want to move Chain A (and B) to match AlphaFold scFv1\n",
        "        sup1 = Superimposer()\n",
        "        sup1.set_atoms(af_scfv1_ca, chain_A_ca)  # fixed=AF, moving=ChainA\n",
        "        result['scFv1_Alignment_RMSD'] = sup1.rms\n",
        "\n",
        "        # Get Chain B target atom coordinates and apply transformation\n",
        "        chain_B_coord = chain_B_target_ca.get_coord().copy()\n",
        "        # Apply rotation and translation\n",
        "        chain_B_coord_transformed = np.dot(chain_B_coord, sup1.rotran[0].T) + sup1.rotran[1]\n",
        "\n",
        "        # === Superimpose Chain C onto AlphaFold scFv2 ===\n",
        "        sup2 = Superimposer()\n",
        "        sup2.set_atoms(af_scfv2_ca, chain_C_ca)  # fixed=AF, moving=ChainC\n",
        "        result['scFv2_Alignment_RMSD'] = sup2.rms\n",
        "\n",
        "        # Get Chain D target atom coordinates and apply transformation\n",
        "        chain_D_coord = chain_D_target_ca.get_coord().copy()\n",
        "        # Apply rotation and translation\n",
        "        chain_D_coord_transformed = np.dot(chain_D_coord, sup2.rotran[0].T) + sup2.rotran[1]\n",
        "\n",
        "        # === Calculate distance ===\n",
        "        antigen_distance = np.linalg.norm(chain_B_coord_transformed - chain_D_coord_transformed)\n",
        "\n",
        "        result['Antigen_Distance'] = antigen_distance\n",
        "        result['Distance_From_Target'] = antigen_distance - TARGET_DISTANCE\n",
        "        result['Chain_B_Transformed'] = chain_B_coord_transformed\n",
        "        result['Chain_D_Transformed'] = chain_D_coord_transformed\n",
        "\n",
        "    except Exception as e:\n",
        "        result['Error'] = str(e)\n",
        "\n",
        "    return result\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZE ALPHAFOLD PREDICTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYZING ALPHAFOLD PREDICTIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load reference structure\n",
        "print(\"\\n--- Loading Reference Structure ---\")\n",
        "ref_scfv1, ref_scfv2 = load_reference_structure(REFERENCE_PATH)\n",
        "\n",
        "# Find all AlphaFold PDB files\n",
        "af_output_path = Path(ALPHAFOLD_OUTPUT_DIR)\n",
        "\n",
        "af_pdb_files = sorted(af_output_path.glob(\"*_relaxed_rank_001.pdb\"))\n",
        "if not af_pdb_files:\n",
        "    af_pdb_files = sorted(af_output_path.glob(\"*_unrelaxed_rank_001.pdb\"))\n",
        "if not af_pdb_files:\n",
        "    af_pdb_files = sorted(af_output_path.glob(\"*rank_001*.pdb\"))\n",
        "if not af_pdb_files:\n",
        "    af_pdb_files = sorted(af_output_path.glob(\"*.pdb\"))\n",
        "\n",
        "print(f\"\\n--- Found {len(af_pdb_files)} AlphaFold PDB Files ---\")\n",
        "\n",
        "if len(af_pdb_files) == 0:\n",
        "    raise RuntimeError(\"No PDB files to analyze\")\n",
        "\n",
        "# Analyze each AlphaFold prediction\n",
        "af_rmsd_results = []\n",
        "\n",
        "for i, pdb_file in enumerate(af_pdb_files, 1):\n",
        "    jobname = pdb_file.stem.replace('_relaxed_rank_001', '').replace('_unrelaxed_rank_001', '')\n",
        "\n",
        "    print(f\"\\n  [{i}/{len(af_pdb_files)}] {jobname}\")\n",
        "\n",
        "    # RMSD analysis\n",
        "    result = analyze_single_target(ref_scfv1, ref_scfv2, str(pdb_file))\n",
        "    result['Jobname'] = jobname\n",
        "    result['AlphaFold_PDB'] = str(pdb_file)\n",
        "\n",
        "    if result['Status'] == 'Success':\n",
        "        print(f\"      RMSD: {result['Final_RMSD']:.4f} Å ({result['Best_Alignment']})\")\n",
        "    else:\n",
        "        print(f\"      RMSD: FAILED - {result['Error']}\")\n",
        "\n",
        "    # Antigen distance analysis\n",
        "    if ANTIGEN_COMPLEX_AVAILABLE:\n",
        "        antigen_result = calculate_antigen_distance(str(pdb_file))\n",
        "        result.update(antigen_result)\n",
        "\n",
        "        if antigen_result['Antigen_Distance'] is not None:\n",
        "            dist = antigen_result['Antigen_Distance']\n",
        "            diff = antigen_result['Distance_From_Target']\n",
        "            print(f\"      Antigen Distance: {dist:.2f} Å (Δ from 130Å: {diff:+.2f} Å)\")\n",
        "\n",
        "    af_rmsd_results.append(result)\n",
        "\n",
        "af_rmsd_df = pd.DataFrame(af_rmsd_results)\n",
        "\n",
        "# =============================================================================\n",
        "# MERGE WITH ORIGINAL DATA\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MERGING ORIGINAL & ALPHAFOLD RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "original_data = {}\n",
        "\n",
        "if filtered_df is not None:\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        if 'format_jobname' in dir():\n",
        "            jobname = format_jobname(row['Filename'])\n",
        "        else:\n",
        "            jobname = row['Filename'].replace('.pdb', '')\n",
        "        original_data[jobname] = {\n",
        "            'Original_RMSD': row['Final_RMSD'],\n",
        "            'Linker_Group': row.get('Linker_Group', 'Unknown'),\n",
        "            'Original_Filename': row['Filename']\n",
        "        }\n",
        "elif 'Original_RMSD' in alphafold_results_df.columns:\n",
        "    for _, row in alphafold_results_df.iterrows():\n",
        "        original_data[row['Jobname']] = {\n",
        "            'Original_RMSD': row['Original_RMSD'],\n",
        "            'Linker_Group': row.get('Linker_Group', 'Unknown'),\n",
        "            'Original_Filename': row.get('Original_Filename', row['Jobname'])\n",
        "        }\n",
        "\n",
        "# Build comparison DataFrame\n",
        "comparison_data = []\n",
        "\n",
        "for _, af_row in af_rmsd_df.iterrows():\n",
        "    if af_row['Status'] != 'Success':\n",
        "        continue\n",
        "\n",
        "    jobname = af_row['Jobname']\n",
        "    af_rmsd = af_row['Final_RMSD']\n",
        "\n",
        "    orig = original_data.get(jobname, {})\n",
        "    orig_rmsd = orig.get('Original_RMSD')\n",
        "    linker_group = orig.get('Linker_Group', af_row.get('Linker_Group', 'Unknown'))\n",
        "\n",
        "    row_data = {\n",
        "        'Jobname': jobname,\n",
        "        'Original_RMSD': orig_rmsd,\n",
        "        'AlphaFold_RMSD': af_rmsd,\n",
        "        'Linker_Group': linker_group,\n",
        "        'AF_Best': af_row['Best_Alignment'],\n",
        "        'AlphaFold_PDB': af_row['AlphaFold_PDB']\n",
        "    }\n",
        "\n",
        "    if orig_rmsd is not None:\n",
        "        row_data['RMSD_Change'] = af_rmsd - orig_rmsd\n",
        "        row_data['RMSD_Improvement'] = orig_rmsd - af_rmsd\n",
        "        row_data['Percent_Change'] = (af_rmsd - orig_rmsd) / orig_rmsd * 100 if orig_rmsd != 0 else 0\n",
        "\n",
        "    # Add antigen distance data\n",
        "    if 'Antigen_Distance' in af_row and af_row['Antigen_Distance'] is not None:\n",
        "        row_data['Antigen_Distance'] = af_row['Antigen_Distance']\n",
        "        row_data['Distance_From_Target'] = af_row['Distance_From_Target']\n",
        "        row_data['Abs_Distance_Error'] = abs(af_row['Distance_From_Target'])\n",
        "\n",
        "    comparison_data.append(row_data)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "if comparison_df.empty:\n",
        "    print(\"\\n⚠️ No comparison data available!\")\n",
        "    comparison_df = af_rmsd_df.copy()\n",
        "    if 'AlphaFold_RMSD' not in comparison_df.columns and 'Final_RMSD' in comparison_df.columns:\n",
        "        comparison_df['AlphaFold_RMSD'] = comparison_df['Final_RMSD']\n",
        "\n",
        "# Sort by Antigen Distance error (closest to 130Å first)\n",
        "if 'Abs_Distance_Error' in comparison_df.columns:\n",
        "    comparison_df = comparison_df.sort_values('Abs_Distance_Error').reset_index(drop=True)\n",
        "elif 'AlphaFold_RMSD' in comparison_df.columns:\n",
        "    comparison_df = comparison_df.sort_values('AlphaFold_RMSD').reset_index(drop=True)\n",
        "\n",
        "# =============================================================================\n",
        "# DISPLAY RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPARISON RESULTS (Sorted by Closest to 130Å Target)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "display_df = comparison_df.copy()\n",
        "\n",
        "# Format columns\n",
        "for col in ['Original_RMSD', 'AlphaFold_RMSD']:\n",
        "    if col in display_df.columns:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
        "\n",
        "if 'Antigen_Distance' in display_df.columns:\n",
        "    display_df['Antigen_Distance'] = display_df['Antigen_Distance'].apply(\n",
        "        lambda x: f\"{x:.2f}\" if pd.notna(x) else \"N/A\"\n",
        "    )\n",
        "if 'Distance_From_Target' in display_df.columns:\n",
        "    display_df['Distance_From_Target'] = display_df['Distance_From_Target'].apply(\n",
        "        lambda x: f\"{x:+.2f}\" if pd.notna(x) else \"N/A\"\n",
        "    )\n",
        "\n",
        "# Select columns to display\n",
        "display_cols = ['Jobname', 'Antigen_Distance', 'Distance_From_Target',\n",
        "                'AlphaFold_RMSD', 'Original_RMSD', 'Linker_Group']\n",
        "display_cols = [c for c in display_cols if c in display_df.columns]\n",
        "\n",
        "print(\"\\n--- Results Table ---\")\n",
        "print(display_df[display_cols].to_string(index=False))\n",
        "\n",
        "# =============================================================================\n",
        "# STATISTICS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STATISTICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if 'Antigen_Distance' in comparison_df.columns and comparison_df['Antigen_Distance'].notna().any():\n",
        "    antigen_dist = comparison_df['Antigen_Distance'].dropna()\n",
        "    print(f\"\\nAntigen Distance Statistics (Target: {TARGET_DISTANCE} Å):\")\n",
        "    print(f\"  • Mean: {antigen_dist.mean():.2f} Å\")\n",
        "    print(f\"  • Std:  {antigen_dist.std():.2f} Å\")\n",
        "    print(f\"  • Min:  {antigen_dist.min():.2f} Å\")\n",
        "    print(f\"  • Max:  {antigen_dist.max():.2f} Å\")\n",
        "    print(f\"  • Closest to target: {antigen_dist.iloc[(antigen_dist - TARGET_DISTANCE).abs().argmin()]:.2f} Å\")\n",
        "\n",
        "if 'AlphaFold_RMSD' in comparison_df.columns:\n",
        "    print(f\"\\nAlphaFold RMSD Statistics:\")\n",
        "    print(f\"  • Mean: {comparison_df['AlphaFold_RMSD'].mean():.4f} Å\")\n",
        "    print(f\"  • Std:  {comparison_df['AlphaFold_RMSD'].std():.4f} Å\")\n",
        "    print(f\"  • Min:  {comparison_df['AlphaFold_RMSD'].min():.4f} Å\")\n",
        "    print(f\"  • Max:  {comparison_df['AlphaFold_RMSD'].max():.4f} Å\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GENERATING VISUALIZATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n_plots = 3 if ANTIGEN_COMPLEX_AVAILABLE and 'Antigen_Distance' in comparison_df.columns else 2\n",
        "fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n",
        "\n",
        "if n_plots == 2:\n",
        "    axes = [axes[0], axes[1], None]\n",
        "\n",
        "# Plot 1: Antigen Distance Distribution\n",
        "if ANTIGEN_COMPLEX_AVAILABLE and 'Antigen_Distance' in comparison_df.columns:\n",
        "    ax1 = axes[0]\n",
        "    distances = comparison_df['Antigen_Distance'].dropna()\n",
        "\n",
        "    bars = ax1.bar(range(len(distances)), distances.values,\n",
        "                   color=['green' if abs(d - TARGET_DISTANCE) < 10 else 'orange' if abs(d - TARGET_DISTANCE) < 20 else 'red'\n",
        "                          for d in distances.values])\n",
        "    ax1.axhline(y=TARGET_DISTANCE, color='blue', linestyle='--', linewidth=2, label=f'Target ({TARGET_DISTANCE}Å)')\n",
        "    ax1.axhspan(TARGET_DISTANCE-10, TARGET_DISTANCE+10, alpha=0.2, color='green', label='±10Å')\n",
        "    ax1.set_xlabel('Sample')\n",
        "    ax1.set_ylabel('Antigen Distance (Å)')\n",
        "    ax1.set_title('Antigen Distance vs Target (130Å)')\n",
        "    ax1.set_xticks(range(len(distances)))\n",
        "    ax1.set_xticklabels(comparison_df['Jobname'].head(len(distances)), rotation=45, ha='right', fontsize=7)\n",
        "    ax1.legend(fontsize=8)\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: RMSD comparison\n",
        "ax2 = axes[1] if axes[1] is not None else axes[0]\n",
        "if 'AlphaFold_RMSD' in comparison_df.columns:\n",
        "    x = np.arange(len(comparison_df))\n",
        "    width = 0.35\n",
        "\n",
        "    if 'Original_RMSD' in comparison_df.columns and comparison_df['Original_RMSD'].notna().any():\n",
        "        bars1 = ax2.bar(x - width/2, comparison_df['Original_RMSD'], width, label='Original', color='#3498db', alpha=0.8)\n",
        "        bars2 = ax2.bar(x + width/2, comparison_df['AlphaFold_RMSD'], width, label='AlphaFold', color='#e74c3c', alpha=0.8)\n",
        "    else:\n",
        "        bars2 = ax2.bar(x, comparison_df['AlphaFold_RMSD'], width, label='AlphaFold', color='#e74c3c', alpha=0.8)\n",
        "\n",
        "    ax2.set_xlabel('Sample')\n",
        "    ax2.set_ylabel('RMSD (Å)')\n",
        "    ax2.set_title('RMSD Comparison')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(comparison_df['Jobname'], rotation=45, ha='right', fontsize=7)\n",
        "    ax2.legend()\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Antigen Distance vs RMSD scatter\n",
        "if ANTIGEN_COMPLEX_AVAILABLE and 'Antigen_Distance' in comparison_df.columns and axes[2] is not None:\n",
        "    ax3 = axes[2]\n",
        "    scatter = ax3.scatter(comparison_df['AlphaFold_RMSD'],\n",
        "                         comparison_df['Antigen_Distance'],\n",
        "                         c=comparison_df['Abs_Distance_Error'],\n",
        "                         cmap='RdYlGn_r', s=100, edgecolors='black')\n",
        "    ax3.axhline(y=TARGET_DISTANCE, color='blue', linestyle='--', alpha=0.7, label=f'Target ({TARGET_DISTANCE}Å)')\n",
        "    ax3.set_xlabel('AlphaFold RMSD (Å)')\n",
        "    ax3.set_ylabel('Antigen Distance (Å)')\n",
        "    ax3.set_title('RMSD vs Antigen Distance')\n",
        "    plt.colorbar(scatter, ax=ax3, label='|Distance - 130Å|')\n",
        "    ax3.legend()\n",
        "\n",
        "    for _, row in comparison_df.iterrows():\n",
        "        if pd.notna(row.get('Antigen_Distance')):\n",
        "            ax3.annotate(row['Jobname'],\n",
        "                        (row['AlphaFold_RMSD'], row['Antigen_Distance']),\n",
        "                        fontsize=6, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(ALPHAFOLD_OUTPUT_DIR) / \"rmsd_antigen_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"  ✓ Saved: rmsd_antigen_comparison.png\")\n",
        "\n",
        "# =============================================================================\n",
        "# TOP CANDIDATES\n",
        "# =============================================================================\n",
        "\n",
        "if 'Antigen_Distance' in comparison_df.columns and comparison_df['Antigen_Distance'].notna().any():\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"TOP 5 CANDIDATES (Closest to {TARGET_DISTANCE}Å Target)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    top5 = comparison_df.head(5)\n",
        "\n",
        "    for rank, (_, row) in enumerate(top5.iterrows(), 1):\n",
        "        print(f\"\\n#{rank}: {row['Jobname']}\")\n",
        "        print(f\"    Antigen Distance: {row['Antigen_Distance']:.2f} Å (Δ: {row['Distance_From_Target']:+.2f} Å)\")\n",
        "        print(f\"    AlphaFold RMSD: {row['AlphaFold_RMSD']:.4f} Å\")\n",
        "        if pd.notna(row.get('Original_RMSD')):\n",
        "            print(f\"    Original RMSD: {row['Original_RMSD']:.4f} Å\")\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 5 Complete\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nData saved to: comparison_df ({len(comparison_df)} rows)\")\n",
        "print(\"\\nKey columns added:\")\n",
        "print(\"  • Antigen_Distance: Distance between CD3e and HER2 membrane residues\")\n",
        "print(\"  • Distance_From_Target: How far from 130Å target\")\n",
        "print(\"  • Abs_Distance_Error: Absolute error from target\")\n",
        "print(\"\\n→ Run Cell 6 for NMA analysis\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFbGZGhEw2ry"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 6: NORMAL MODE ANALYSIS (NMA)\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 6: Normal Mode Analysis (NMA)** { display-mode: \"form\" }\n",
        "#@markdown ### NMA Parameters\n",
        "GNM_CUTOFF = 10.0  #@param {type:\"number\"}\n",
        "ANM_CUTOFF = 15.0  #@param {type:\"number\"}\n",
        "N_MODES = 20  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Metrics calculated:**\n",
        "#@markdown - Effective Stiffness (GNM)\n",
        "#@markdown - Mean Square Fluctuation (MSF)\n",
        "#@markdown - Deformability Index (ANM)\n",
        "#@markdown - Inter-Domain Distance Fluctuation\n",
        "#@markdown\n",
        "#@markdown **Requires:** Cells 1-4 must be run first.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 6: Normal Mode Analysis (NMA)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Install ProDy if needed\n",
        "try:\n",
        "    import prody\n",
        "    from prody import *\n",
        "except ImportError:\n",
        "    print(\"Installing ProDy...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', '-q', 'prody'], capture_output=True)\n",
        "    import prody\n",
        "    from prody import *\n",
        "\n",
        "prody.confProDy(verbosity='warning')\n",
        "print(f\"ProDy Version: {prody.__version__}\")\n",
        "\n",
        "# Domain constants\n",
        "SCFV1_LENGTH = 240\n",
        "SCFV2_LENGTH = 242\n",
        "ALPHAFOLD_OUTPUT_DIR = \"/content/alphafold_results/\"\n",
        "\n",
        "print(f\"\\nDomain Structure:\")\n",
        "print(f\"  • scFv1: {SCFV1_LENGTH} residues\")\n",
        "print(f\"  • scFv2: {SCFV2_LENGTH} residues\")\n",
        "\n",
        "# =============================================================================\n",
        "# NMA ANALYZER CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class LinkerFlexibilityAnalyzer:\n",
        "    \"\"\"Flexibility analysis for rigid linker validation using GNM/ANM.\"\"\"\n",
        "\n",
        "    def __init__(self, structure_path: str, name: str = None):\n",
        "        self.name = name or os.path.basename(structure_path).replace('.pdb', '')\n",
        "        self.structure = parsePDB(structure_path)\n",
        "        self.calphas = self.structure.select('calpha')\n",
        "\n",
        "        if self.calphas is None:\n",
        "            raise ValueError(f\"No CA atoms found in {structure_path}\")\n",
        "\n",
        "        self.n_residues = self.calphas.numAtoms()\n",
        "        self.domain_indices = self._calculate_domain_indices()\n",
        "        self.linker_length = self.domain_indices['linker_length']\n",
        "\n",
        "        self._gnm = None\n",
        "        self._anm = None\n",
        "        self.results = {}\n",
        "\n",
        "    def _calculate_domain_indices(self) -> Dict:\n",
        "        linker_length = self.n_residues - SCFV1_LENGTH - SCFV2_LENGTH\n",
        "        if linker_length <= 0:\n",
        "            raise ValueError(f\"Invalid total length {self.n_residues}\")\n",
        "\n",
        "        return {\n",
        "            'scfv1': (0, SCFV1_LENGTH),\n",
        "            'linker': (SCFV1_LENGTH, SCFV1_LENGTH + linker_length),\n",
        "            'scfv2': (SCFV1_LENGTH + linker_length, self.n_residues),\n",
        "            'linker_length': linker_length\n",
        "        }\n",
        "\n",
        "    def run_gnm(self, cutoff: float = 10.0, n_modes: int = 20):\n",
        "        self._gnm = GNM(self.name)\n",
        "        self._gnm.buildKirchhoff(self.calphas, cutoff=cutoff)\n",
        "        self._gnm.calcModes(n_modes=n_modes)\n",
        "        return self._gnm\n",
        "\n",
        "    def calc_msf_gnm(self) -> np.ndarray:\n",
        "        if self._gnm is None:\n",
        "            self.run_gnm()\n",
        "\n",
        "        msf = calcSqFlucts(self._gnm)\n",
        "        self.results['msf_gnm'] = msf\n",
        "        self.results['msf_gnm_mean'] = np.mean(msf)\n",
        "        self.results['msf_gnm_std'] = np.std(msf)\n",
        "\n",
        "        # Domain-specific MSF\n",
        "        for domain, (start, end) in [('scfv1', self.domain_indices['scfv1']),\n",
        "                                      ('scfv2', self.domain_indices['scfv2']),\n",
        "                                      ('linker', self.domain_indices['linker'])]:\n",
        "            domain_msf = msf[start:end]\n",
        "            self.results[f'{domain}_msf_mean'] = np.mean(domain_msf)\n",
        "            self.results[f'{domain}_msf_std'] = np.std(domain_msf)\n",
        "            self.results[f'{domain}_msf_max'] = np.max(domain_msf)\n",
        "            self.results[f'{domain}_msf_min'] = np.min(domain_msf)\n",
        "\n",
        "        return msf\n",
        "\n",
        "    def run_anm(self, cutoff: float = 15.0, n_modes: int = 20):\n",
        "        self._anm = ANM(self.name)\n",
        "        self._anm.buildHessian(self.calphas, cutoff=cutoff)\n",
        "        self._anm.calcModes(n_modes=n_modes)\n",
        "        return self._anm\n",
        "\n",
        "    def calc_deformability(self, n_modes: int = None) -> np.ndarray:\n",
        "        if self._anm is None:\n",
        "            self.run_anm()\n",
        "\n",
        "        if n_modes is None:\n",
        "            n_modes = len(self._anm)\n",
        "\n",
        "        eigenvalues = self._anm.getEigvals()[:n_modes]\n",
        "        eigenvectors = self._anm.getEigvecs()[:, :n_modes]\n",
        "\n",
        "        eigenvectors_reshaped = eigenvectors.reshape(self.n_residues, 3, n_modes)\n",
        "        u_squared = np.sum(eigenvectors_reshaped**2, axis=1)\n",
        "        deformability_raw = np.sum(u_squared / eigenvalues**2, axis=1)\n",
        "\n",
        "        # Normalize to 0-1\n",
        "        deformability = (deformability_raw - deformability_raw.min()) / \\\n",
        "                       (deformability_raw.max() - deformability_raw.min())\n",
        "\n",
        "        self.results['deformability'] = deformability\n",
        "        self.results['deformability_mean'] = np.mean(deformability)\n",
        "        self.results['deformability_std'] = np.std(deformability)\n",
        "\n",
        "        for domain, (start, end) in [('scfv1', self.domain_indices['scfv1']),\n",
        "                                      ('scfv2', self.domain_indices['scfv2']),\n",
        "                                      ('linker', self.domain_indices['linker'])]:\n",
        "            domain_def = deformability[start:end]\n",
        "            self.results[f'{domain}_deformability_mean'] = np.mean(domain_def)\n",
        "            self.results[f'{domain}_deformability_max'] = np.max(domain_def)\n",
        "            self.results[f'{domain}_deformability_min'] = np.min(domain_def)\n",
        "\n",
        "        return deformability\n",
        "\n",
        "    def calc_stiffness(self) -> float:\n",
        "        if self._gnm is None:\n",
        "            self.run_gnm()\n",
        "\n",
        "        eigenvalues = self._gnm.getEigvals()[:10]\n",
        "        stiffness = np.mean(eigenvalues)\n",
        "\n",
        "        self.results['stiffness'] = stiffness\n",
        "        self.results['stiffness_min_eigenvalue'] = eigenvalues[0]\n",
        "        self.results['stiffness_max_eigenvalue'] = eigenvalues[-1]\n",
        "\n",
        "        return stiffness\n",
        "\n",
        "    def calc_inter_domain_distance_fluctuation(self, n_modes: int = 20) -> float:\n",
        "        if self._anm is None:\n",
        "            self.run_anm(n_modes=n_modes)\n",
        "\n",
        "        coords = self.calphas.getCoords()\n",
        "        scfv1_start, scfv1_end = self.domain_indices['scfv1']\n",
        "        scfv2_start, scfv2_end = self.domain_indices['scfv2']\n",
        "\n",
        "        com1_initial = np.mean(coords[scfv1_start:scfv1_end], axis=0)\n",
        "        com2_initial = np.mean(coords[scfv2_start:scfv2_end], axis=0)\n",
        "        initial_distance = np.linalg.norm(com2_initial - com1_initial)\n",
        "\n",
        "        eigenvectors = self._anm.getEigvecs()\n",
        "        eigenvalues = self._anm.getEigvals()\n",
        "        n_modes_available = min(n_modes, len(self._anm))\n",
        "\n",
        "        distance_changes = []\n",
        "        for mode_idx in range(n_modes_available):\n",
        "            mode_vector = eigenvectors[:, mode_idx].reshape(-1, 3)\n",
        "            amplitude = 5.0 / np.sqrt(eigenvalues[mode_idx])\n",
        "            displaced_coords = coords + amplitude * mode_vector\n",
        "\n",
        "            com1_displaced = np.mean(displaced_coords[scfv1_start:scfv1_end], axis=0)\n",
        "            com2_displaced = np.mean(displaced_coords[scfv2_start:scfv2_end], axis=0)\n",
        "            new_distance = np.linalg.norm(com2_displaced - com1_displaced)\n",
        "            distance_changes.append(new_distance - initial_distance)\n",
        "\n",
        "        weights = 1.0 / eigenvalues[:n_modes_available]\n",
        "        weights = weights / np.sum(weights)\n",
        "        mean_change = np.average(distance_changes, weights=weights)\n",
        "        variance = np.average((np.array(distance_changes) - mean_change)**2, weights=weights)\n",
        "        inter_domain_fluctuation = np.sqrt(variance)\n",
        "\n",
        "        self.results['inter_domain_distance_initial'] = initial_distance\n",
        "        self.results['inter_domain_fluctuation'] = inter_domain_fluctuation\n",
        "        self.results['inter_domain_fluctuation_simple'] = np.std(distance_changes)\n",
        "\n",
        "        return inter_domain_fluctuation\n",
        "\n",
        "    def run_full_analysis(self, gnm_cutoff: float = 10.0, anm_cutoff: float = 15.0, n_modes: int = 20):\n",
        "        self.run_gnm(cutoff=gnm_cutoff, n_modes=n_modes)\n",
        "        self.calc_msf_gnm()\n",
        "        self.calc_stiffness()\n",
        "\n",
        "        self.run_anm(cutoff=anm_cutoff, n_modes=n_modes)\n",
        "        self.calc_deformability(n_modes=n_modes)\n",
        "        self.calc_inter_domain_distance_fluctuation(n_modes=n_modes)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def get_summary_dict(self) -> dict:\n",
        "        return {\n",
        "            'Filename': self.name,\n",
        "            'Total_Residues': self.n_residues,\n",
        "            'Linker_Length': self.linker_length,\n",
        "            'Effective_Stiffness': self.results.get('stiffness', np.nan),\n",
        "            'Min_Eigenvalue': self.results.get('stiffness_min_eigenvalue', np.nan),\n",
        "            'Max_Eigenvalue_10': self.results.get('stiffness_max_eigenvalue', np.nan),\n",
        "            'Inter_Domain_Fluctuation': self.results.get('inter_domain_fluctuation', np.nan),\n",
        "            'Inter_Domain_Fluctuation_Simple': self.results.get('inter_domain_fluctuation_simple', np.nan),\n",
        "            'Initial_COM_Distance': self.results.get('inter_domain_distance_initial', np.nan),\n",
        "            'MSF_Global_Mean': self.results.get('msf_gnm_mean', np.nan),\n",
        "            'MSF_Global_Std': self.results.get('msf_gnm_std', np.nan),\n",
        "            'MSF_scFv1_Mean': self.results.get('scfv1_msf_mean', np.nan),\n",
        "            'MSF_scFv2_Mean': self.results.get('scfv2_msf_mean', np.nan),\n",
        "            'MSF_Linker_Mean': self.results.get('linker_msf_mean', np.nan),\n",
        "            'MSF_Linker_Std': self.results.get('linker_msf_std', np.nan),\n",
        "            'MSF_Linker_Max': self.results.get('linker_msf_max', np.nan),\n",
        "            'MSF_Linker_Min': self.results.get('linker_msf_min', np.nan),\n",
        "            'Deformability_Global_Mean': self.results.get('deformability_mean', np.nan),\n",
        "            'Deformability_Global_Std': self.results.get('deformability_std', np.nan),\n",
        "            'Deformability_scFv1_Mean': self.results.get('scfv1_deformability_mean', np.nan),\n",
        "            'Deformability_scFv2_Mean': self.results.get('scfv2_deformability_mean', np.nan),\n",
        "            'Deformability_Linker_Mean': self.results.get('linker_deformability_mean', np.nan),\n",
        "            'Deformability_Linker_Max': self.results.get('linker_deformability_max', np.nan),\n",
        "            'Deformability_Linker_Min': self.results.get('linker_deformability_min', np.nan),\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# BATCH ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Running NMA Analysis...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "try:\n",
        "    _ = alphafold_results_df\n",
        "    if len(alphafold_results_df) == 0:\n",
        "        raise ValueError(\"Empty\")\n",
        "except:\n",
        "    raise RuntimeError(\"alphafold_results_df not found. Run Cell 4 first.\")\n",
        "\n",
        "nma_results = []\n",
        "\n",
        "for idx, row in alphafold_results_df.iterrows():\n",
        "    jobname = row['Jobname']\n",
        "    pdb_path = row['AlphaFold_PDB']\n",
        "\n",
        "    print(f\"\\n[{idx+1}/{len(alphafold_results_df)}] Analyzing: {jobname}\")\n",
        "\n",
        "    if not os.path.exists(pdb_path):\n",
        "        print(f\"  ✗ PDB not found: {pdb_path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        analyzer = LinkerFlexibilityAnalyzer(pdb_path, name=jobname)\n",
        "        analyzer.run_full_analysis(gnm_cutoff=GNM_CUTOFF, anm_cutoff=ANM_CUTOFF, n_modes=N_MODES)\n",
        "\n",
        "        summary = analyzer.get_summary_dict()\n",
        "        summary['Original_Filename'] = row.get('Original_Filename', jobname)\n",
        "        summary['Original_RMSD'] = row.get('Original_RMSD', np.nan)\n",
        "        summary['Linker_Group'] = row.get('Linker_Group', 'Unknown')\n",
        "\n",
        "        nma_results.append(summary)\n",
        "\n",
        "        print(f\"  ✓ Stiffness: {summary['Effective_Stiffness']:.6f}\")\n",
        "        print(f\"  ✓ Inter-Domain Fluct: {summary['Inter_Domain_Fluctuation']:.4f} Å\")\n",
        "        print(f\"  ✓ Linker MSF: {summary['MSF_Linker_Mean']:.4f} Ų\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "if nma_results:\n",
        "    nma_results_df = pd.DataFrame(nma_results)\n",
        "    nma_results_df.to_csv(Path(ALPHAFOLD_OUTPUT_DIR) / \"nma_analysis_results.csv\", index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"NMA ANALYSIS COMPLETE: {len(nma_results)} structures\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\n--- NMA Summary ---\")\n",
        "    display_cols = ['Filename', 'Linker_Length', 'Effective_Stiffness',\n",
        "                   'Inter_Domain_Fluctuation', 'MSF_Linker_Mean', 'Deformability_Linker_Mean']\n",
        "    display_df = nma_results_df[display_cols].copy()\n",
        "\n",
        "    for col in ['Effective_Stiffness']:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.6f}\")\n",
        "    for col in ['Inter_Domain_Fluctuation', 'MSF_Linker_Mean', 'Deformability_Linker_Mean']:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "    print(display_df.to_string(index=False))\n",
        "else:\n",
        "    nma_results_df = pd.DataFrame()\n",
        "    print(\"\\n⚠️ No NMA results obtained\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 6 Complete\")\n",
        "print(\"=\" * 70)\n",
        "print(\"→ Run Cell 7 for pLDDT/PAE analysis\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSTqCc4Dw2rz"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 7: ALPHAFOLD JSON ANALYSIS (pLDDT & PAE)\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 7: pLDDT & PAE Analysis** { display-mode: \"form\" }\n",
        "#@markdown Analyzes AlphaFold JSON output files for confidence metrics.\n",
        "#@markdown\n",
        "#@markdown **Metrics calculated:**\n",
        "#@markdown - pLDDT scores (global, domain-specific, junction)\n",
        "#@markdown - PAE metrics (inter-domain, linker stability)\n",
        "#@markdown - pTM/ipTM scores\n",
        "#@markdown\n",
        "#@markdown **Requires:** Cell 4 must be run first.\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 7: AlphaFold JSON Analysis (pLDDT & PAE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Domain constants\n",
        "SCFV1_LENGTH = 240\n",
        "SCFV2_LENGTH = 242\n",
        "ALPHAFOLD_OUTPUT_DIR = \"/content/alphafold_results/\"\n",
        "\n",
        "print(f\"\\nDomain Structure:\")\n",
        "print(f\"  • scFv1: {SCFV1_LENGTH} residues\")\n",
        "print(f\"  • scFv2: {SCFV2_LENGTH} residues\")\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYSIS FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def load_alphafold_json(filepath: str) -> Optional[Dict]:\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def calculate_domain_indices(total_length: int) -> Dict:\n",
        "    linker_length = total_length - SCFV1_LENGTH - SCFV2_LENGTH\n",
        "    if linker_length <= 0:\n",
        "        raise ValueError(f\"Invalid total length {total_length}\")\n",
        "\n",
        "    return {\n",
        "        'scfv1': (0, SCFV1_LENGTH),\n",
        "        'linker': (SCFV1_LENGTH, SCFV1_LENGTH + linker_length),\n",
        "        'scfv2': (SCFV1_LENGTH + linker_length, total_length),\n",
        "        'linker_length': linker_length,\n",
        "        'junction_n': (SCFV1_LENGTH, SCFV1_LENGTH + 3),\n",
        "        'junction_c': (SCFV1_LENGTH + linker_length - 3, SCFV1_LENGTH + linker_length)\n",
        "    }\n",
        "\n",
        "\n",
        "def extract_plddt_metrics(plddt: List[float], indices: Dict) -> Dict[str, float]:\n",
        "    plddt_array = np.array(plddt)\n",
        "\n",
        "    scfv1_plddt = plddt_array[indices['scfv1'][0]:indices['scfv1'][1]]\n",
        "    scfv2_plddt = plddt_array[indices['scfv2'][0]:indices['scfv2'][1]]\n",
        "    linker_plddt = plddt_array[indices['linker'][0]:indices['linker'][1]]\n",
        "\n",
        "    junction_plddt = np.concatenate([\n",
        "        plddt_array[indices['junction_n'][0]:indices['junction_n'][1]],\n",
        "        plddt_array[indices['junction_c'][0]:indices['junction_c'][1]]\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        'pLDDT_Global_Mean': float(np.mean(plddt_array)),\n",
        "        'pLDDT_Global_Std': float(np.std(plddt_array)),\n",
        "        'pLDDT_Global_Min': float(np.min(plddt_array)),\n",
        "        'pLDDT_Global_Max': float(np.max(plddt_array)),\n",
        "        'pLDDT_scFv1_Mean': float(np.mean(scfv1_plddt)),\n",
        "        'pLDDT_scFv1_Std': float(np.std(scfv1_plddt)),\n",
        "        'pLDDT_scFv1_Min': float(np.min(scfv1_plddt)),\n",
        "        'pLDDT_scFv2_Mean': float(np.mean(scfv2_plddt)),\n",
        "        'pLDDT_scFv2_Std': float(np.std(scfv2_plddt)),\n",
        "        'pLDDT_scFv2_Min': float(np.min(scfv2_plddt)),\n",
        "        'pLDDT_Linker_Mean': float(np.mean(linker_plddt)),\n",
        "        'pLDDT_Linker_Std': float(np.std(linker_plddt)),\n",
        "        'pLDDT_Linker_Min': float(np.min(linker_plddt)),\n",
        "        'pLDDT_Linker_Max': float(np.max(linker_plddt)),\n",
        "        'pLDDT_Junction_Mean': float(np.mean(junction_plddt)),\n",
        "        'pLDDT_Junction_Min': float(np.min(junction_plddt)),\n",
        "        'pLDDT_Junction_N_Mean': float(np.mean(plddt_array[indices['junction_n'][0]:indices['junction_n'][1]])),\n",
        "        'pLDDT_Junction_C_Mean': float(np.mean(plddt_array[indices['junction_c'][0]:indices['junction_c'][1]])),\n",
        "    }\n",
        "\n",
        "\n",
        "def extract_pae_metrics(pae: List[List[float]], indices: Dict) -> Dict[str, float]:\n",
        "    pae_array = np.array(pae)\n",
        "\n",
        "    scfv1_s, scfv1_e = indices['scfv1']\n",
        "    scfv2_s, scfv2_e = indices['scfv2']\n",
        "    linker_s, linker_e = indices['linker']\n",
        "\n",
        "    pae_12 = pae_array[scfv1_s:scfv1_e, scfv2_s:scfv2_e]\n",
        "    pae_21 = pae_array[scfv2_s:scfv2_e, scfv1_s:scfv1_e]\n",
        "    inter_domain = (np.mean(pae_12) + np.mean(pae_21)) / 2\n",
        "\n",
        "    linker_to_1 = np.mean(pae_array[linker_s:linker_e, scfv1_s:scfv1_e])\n",
        "    linker_to_2 = np.mean(pae_array[linker_s:linker_e, scfv2_s:scfv2_e])\n",
        "\n",
        "    pae_linker = pae_array[linker_s:linker_e, linker_s:linker_e]\n",
        "    mask = ~np.eye(linker_e - linker_s, dtype=bool)\n",
        "    linker_internal = np.mean(pae_linker[mask]) if mask.sum() > 0 else 0.0\n",
        "\n",
        "    pae_scfv1 = pae_array[scfv1_s:scfv1_e, scfv1_s:scfv1_e]\n",
        "    mask1 = ~np.eye(scfv1_e - scfv1_s, dtype=bool)\n",
        "    scfv1_internal = np.mean(pae_scfv1[mask1])\n",
        "\n",
        "    pae_scfv2 = pae_array[scfv2_s:scfv2_e, scfv2_s:scfv2_e]\n",
        "    mask2 = ~np.eye(scfv2_e - scfv2_s, dtype=bool)\n",
        "    scfv2_internal = np.mean(pae_scfv2[mask2])\n",
        "\n",
        "    junction_n = np.mean(pae_array[linker_s:linker_s+5, scfv1_e-10:scfv1_e])\n",
        "    junction_c = np.mean(pae_array[linker_e-5:linker_e, scfv2_s:scfv2_s+10])\n",
        "\n",
        "    return {\n",
        "        'PAE_Inter_Domain': float(inter_domain),\n",
        "        'PAE_scFv1_to_scFv2': float(np.mean(pae_12)),\n",
        "        'PAE_scFv2_to_scFv1': float(np.mean(pae_21)),\n",
        "        'PAE_Linker_to_scFv1': float(linker_to_1),\n",
        "        'PAE_Linker_to_scFv2': float(linker_to_2),\n",
        "        'PAE_Linker_Stability': float((linker_to_1 + linker_to_2) / 2),\n",
        "        'PAE_Linker_Internal': float(linker_internal),\n",
        "        'PAE_scFv1_Internal': float(scfv1_internal),\n",
        "        'PAE_scFv2_Internal': float(scfv2_internal),\n",
        "        'PAE_Junction_N': float(junction_n),\n",
        "        'PAE_Junction_C': float(junction_c),\n",
        "        'PAE_Junction_Mean': float((junction_n + junction_c) / 2),\n",
        "        'PAE_Global_Mean': float(np.mean(pae_array)),\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_alphafold_json(filepath: str, filename: str = None) -> Optional[Dict]:\n",
        "    data = load_alphafold_json(filepath)\n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    if filename is None:\n",
        "        filename = Path(filepath).name\n",
        "\n",
        "    plddt = data.get('plddt', [])\n",
        "    if not plddt:\n",
        "        return None\n",
        "\n",
        "    total_length = len(plddt)\n",
        "\n",
        "    try:\n",
        "        indices = calculate_domain_indices(total_length)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "    results = {\n",
        "        'Filename': filename,\n",
        "        'Total_Length': total_length,\n",
        "        'Linker_Length': indices['linker_length'],\n",
        "        'pTM_Score': data.get('ptm', data.get('pTM', data.get('ranking_confidence', np.nan))),\n",
        "        'ipTM_Score': data.get('iptm', data.get('ipTM', np.nan))\n",
        "    }\n",
        "\n",
        "    results.update(extract_plddt_metrics(plddt, indices))\n",
        "\n",
        "    pae = data.get('pae', data.get('predicted_aligned_error', None))\n",
        "    if pae is not None:\n",
        "        results.update(extract_pae_metrics(pae, indices))\n",
        "    else:\n",
        "        for key in ['PAE_Inter_Domain', 'PAE_Linker_Stability', 'PAE_Junction_Mean']:\n",
        "            results[key] = np.nan\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# BATCH ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Running JSON Analysis...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "try:\n",
        "    _ = alphafold_results_df\n",
        "    if len(alphafold_results_df) == 0:\n",
        "        raise ValueError(\"Empty\")\n",
        "except:\n",
        "    raise RuntimeError(\"alphafold_results_df not found. Run Cell 4 first.\")\n",
        "\n",
        "json_results = []\n",
        "\n",
        "for idx, row in alphafold_results_df.iterrows():\n",
        "    jobname = row['Jobname']\n",
        "    json_path = row.get('AlphaFold_JSON', None)\n",
        "\n",
        "    print(f\"\\n[{idx+1}/{len(alphafold_results_df)}] Analyzing: {jobname}\")\n",
        "\n",
        "    if json_path is None or not os.path.exists(str(json_path)):\n",
        "        print(f\"  ✗ JSON not found\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        result = analyze_alphafold_json(str(json_path), filename=jobname)\n",
        "\n",
        "        if result is not None:\n",
        "            result['Original_Filename'] = row.get('Original_Filename', jobname)\n",
        "            result['Original_RMSD'] = row.get('Original_RMSD', np.nan)\n",
        "            result['Linker_Group'] = row.get('Linker_Group', 'Unknown')\n",
        "\n",
        "            json_results.append(result)\n",
        "\n",
        "            ptm = result.get('pTM_Score', np.nan)\n",
        "            ptm_str = f\"{ptm:.3f}\" if pd.notna(ptm) else \"N/A\"\n",
        "            print(f\"  ✓ pTM: {ptm_str}\")\n",
        "            print(f\"  ✓ Global pLDDT: {result['pLDDT_Global_Mean']:.1f}\")\n",
        "            print(f\"  ✓ Linker pLDDT: {result['pLDDT_Linker_Mean']:.1f}\")\n",
        "            if pd.notna(result.get('PAE_Inter_Domain', np.nan)):\n",
        "                print(f\"  ✓ Inter-Domain PAE: {result['PAE_Inter_Domain']:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "if json_results:\n",
        "    json_results_df = pd.DataFrame(json_results)\n",
        "    json_results_df.to_csv(Path(ALPHAFOLD_OUTPUT_DIR) / \"json_analysis_results.csv\", index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"JSON ANALYSIS COMPLETE: {len(json_results)} files\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\n--- pLDDT/PAE Summary ---\")\n",
        "    display_cols = ['Filename', 'Linker_Length', 'pTM_Score',\n",
        "                   'pLDDT_Global_Mean', 'pLDDT_Linker_Mean', 'PAE_Inter_Domain']\n",
        "    display_cols = [c for c in display_cols if c in json_results_df.columns]\n",
        "    display_df = json_results_df[display_cols].copy()\n",
        "\n",
        "    for col in display_df.columns:\n",
        "        if 'pLDDT' in col:\n",
        "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else \"N/A\")\n",
        "        elif 'PAE' in col:\n",
        "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"N/A\")\n",
        "        elif 'pTM' in col:\n",
        "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
        "\n",
        "    print(display_df.to_string(index=False))\n",
        "else:\n",
        "    json_results_df = pd.DataFrame()\n",
        "    print(\"\\n⚠️ No JSON results obtained\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Cell 7 Complete\")\n",
        "print(\"=\" * 70)\n",
        "print(\"→ Run Cell 8 for Final Summary & Download\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGQHkONiw2rz"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 8: FINAL SUMMARY - COMBINED RESULTS & DOWNLOAD\n",
        "# ================================================================================\n",
        "# Combines all results and provides downloadable files:\n",
        "# - Excel file with all metrics (multiple sheets)\n",
        "# - ZIP file containing all PDB, JSON, CSV files\n",
        "# ================================================================================\n",
        "\n",
        "#@title **Cell 8: Final Summary & Download** { display-mode: \"form\" }\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install openpyxl for Excel export\n",
        "try:\n",
        "    import openpyxl\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', '-q', 'openpyxl'], capture_output=True)\n",
        "    import openpyxl\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 8: Final Summary - Combined Results & Download\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "ALPHAFOLD_OUTPUT_DIR = \"/content/alphafold_results/\"\n",
        "DOWNLOAD_DIR = \"/content/download/\"\n",
        "\n",
        "# Create download directory\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# COLLECT ALL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 1/5] Collecting results from all analyses...\")\n",
        "\n",
        "available_dfs = {}\n",
        "\n",
        "# AlphaFold results (from Cell 4)\n",
        "try:\n",
        "    if 'alphafold_results_df' in dir() and len(alphafold_results_df) > 0:\n",
        "        available_dfs['alphafold'] = alphafold_results_df\n",
        "        print(f\"  ✓ AlphaFold results: {len(alphafold_results_df)} entries\")\n",
        "except:\n",
        "    print(\"  ✗ AlphaFold results not found\")\n",
        "\n",
        "# Comparison results (from Cell 5)\n",
        "try:\n",
        "    if 'comparison_df' in dir() and len(comparison_df) > 0:\n",
        "        available_dfs['comparison'] = comparison_df\n",
        "        print(f\"  ✓ RMSD Comparison results: {len(comparison_df)} entries\")\n",
        "except:\n",
        "    print(\"  ✗ RMSD Comparison results not found\")\n",
        "\n",
        "# NMA results (from Cell 6)\n",
        "try:\n",
        "    if 'nma_results_df' in dir() and len(nma_results_df) > 0:\n",
        "        available_dfs['nma'] = nma_results_df\n",
        "        print(f\"  ✓ NMA results: {len(nma_results_df)} entries\")\n",
        "except:\n",
        "    print(\"  ✗ NMA results not found\")\n",
        "\n",
        "# JSON results (from Cell 7)\n",
        "try:\n",
        "    if 'json_results_df' in dir() and len(json_results_df) > 0:\n",
        "        available_dfs['json'] = json_results_df\n",
        "        print(f\"  ✓ JSON (pLDDT/PAE) results: {len(json_results_df)} entries\")\n",
        "except:\n",
        "    print(\"  ✗ JSON results not found\")\n",
        "\n",
        "if not available_dfs:\n",
        "    print(\"\\n✗ No analysis results found!\")\n",
        "    raise RuntimeError(\"No results to combine\")\n",
        "\n",
        "# ============================================================================\n",
        "# MERGE ALL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 2/5] Merging all results...\")\n",
        "\n",
        "# Start with AlphaFold results as base\n",
        "if 'alphafold' in available_dfs:\n",
        "    combined_df = available_dfs['alphafold'].copy()\n",
        "    base_cols = ['Jobname', 'Original_Filename', 'Original_RMSD', 'Linker_Group',\n",
        "                 'Sequence_Length', 'AlphaFold_PDB', 'AlphaFold_JSON', 'PDB_Type', 'Elapsed_Time']\n",
        "    combined_df = combined_df[[c for c in base_cols if c in combined_df.columns]]\n",
        "else:\n",
        "    combined_df = available_dfs.get('nma', available_dfs.get('json', pd.DataFrame()))\n",
        "\n",
        "# Merge Comparison results (Cell 5)\n",
        "if 'comparison' in available_dfs:\n",
        "    comp_cols = ['Jobname', 'AlphaFold_RMSD', 'RMSD_Change', 'RMSD_Improvement', 'Percent_Change']\n",
        "    comp_cols = [c for c in comp_cols if c in available_dfs['comparison'].columns]\n",
        "    if comp_cols:\n",
        "        comp_merge = available_dfs['comparison'][comp_cols].copy()\n",
        "        combined_df = combined_df.merge(comp_merge, on='Jobname', how='left')\n",
        "        print(f\"  ✓ Merged RMSD Comparison results\")\n",
        "\n",
        "# Merge NMA results\n",
        "if 'nma' in available_dfs:\n",
        "    nma_cols_to_merge = [c for c in available_dfs['nma'].columns\n",
        "                        if c not in ['Original_Filename', 'Original_RMSD', 'Linker_Group', 'Total_Residues']]\n",
        "    nma_merge = available_dfs['nma'][nma_cols_to_merge].copy()\n",
        "    nma_merge = nma_merge.rename(columns={'Filename': 'Jobname'})\n",
        "    combined_df = combined_df.merge(nma_merge, on='Jobname', how='left')\n",
        "    print(f\"  ✓ Merged NMA results\")\n",
        "\n",
        "# Merge JSON results\n",
        "if 'json' in available_dfs:\n",
        "    json_cols_to_merge = [c for c in available_dfs['json'].columns\n",
        "                         if c not in ['Original_Filename', 'Original_RMSD', 'Linker_Group', 'Total_Length']]\n",
        "    json_merge = available_dfs['json'][json_cols_to_merge].copy()\n",
        "    json_merge = json_merge.rename(columns={'Filename': 'Jobname'})\n",
        "    combined_df = combined_df.merge(json_merge, on='Jobname', how='left')\n",
        "    print(f\"  ✓ Merged JSON results\")\n",
        "\n",
        "print(f\"\\n  Combined DataFrame: {len(combined_df)} rows × {len(combined_df.columns)} columns\")\n",
        "\n",
        "# ============================================================================\n",
        "# CALCULATE RIGIDITY SCORES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 3/5] Calculating rigidity scores...\")\n",
        "\n",
        "def normalize_0_1(series, higher_is_better=True):\n",
        "    if series.isna().all():\n",
        "        return series\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    if max_val == min_val:\n",
        "        return pd.Series([0.5] * len(series))\n",
        "    normalized = (series - min_val) / (max_val - min_val)\n",
        "    if not higher_is_better:\n",
        "        normalized = 1 - normalized\n",
        "    return normalized\n",
        "\n",
        "rigidity_components = {}\n",
        "\n",
        "if 'Effective_Stiffness' in combined_df.columns:\n",
        "    rigidity_components['Stiffness_Score'] = normalize_0_1(combined_df['Effective_Stiffness'], higher_is_better=True)\n",
        "if 'Inter_Domain_Fluctuation' in combined_df.columns:\n",
        "    rigidity_components['InterDomain_Score'] = normalize_0_1(combined_df['Inter_Domain_Fluctuation'], higher_is_better=False)\n",
        "if 'pLDDT_Linker_Mean' in combined_df.columns:\n",
        "    rigidity_components['pLDDT_Score'] = normalize_0_1(combined_df['pLDDT_Linker_Mean'], higher_is_better=True)\n",
        "if 'PAE_Linker_Stability' in combined_df.columns:\n",
        "    rigidity_components['PAE_Score'] = normalize_0_1(combined_df['PAE_Linker_Stability'], higher_is_better=False)\n",
        "if 'MSF_Linker_Mean' in combined_df.columns:\n",
        "    rigidity_components['MSF_Score'] = normalize_0_1(combined_df['MSF_Linker_Mean'], higher_is_better=False)\n",
        "\n",
        "if rigidity_components:\n",
        "    rigidity_df = pd.DataFrame(rigidity_components)\n",
        "    combined_df['Rigidity_Score'] = rigidity_df.mean(axis=1, skipna=True)\n",
        "    combined_df['Rigidity_Rank'] = combined_df['Rigidity_Score'].rank(ascending=False, method='min').astype(int)\n",
        "    print(f\"  ✓ Calculated Rigidity Score from {len(rigidity_components)} components\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE SUMMARY DATAFRAME (KEY METRICS ONLY)\n",
        "# ============================================================================\n",
        "\n",
        "# Define key columns for summary sheet\n",
        "summary_columns = [\n",
        "    'Jobname', 'Linker_Length', 'Original_RMSD', 'AlphaFold_RMSD', 'RMSD_Improvement',\n",
        "    'Rigidity_Score', 'Rigidity_Rank',\n",
        "    'Effective_Stiffness', 'Inter_Domain_Fluctuation', 'Initial_COM_Distance',\n",
        "    'MSF_Linker_Mean', 'Deformability_Linker_Mean',\n",
        "    'pTM_Score', 'pLDDT_Global_Mean', 'pLDDT_Linker_Mean', 'pLDDT_Linker_Min', 'pLDDT_Junction_Mean',\n",
        "    'PAE_Inter_Domain', 'PAE_Linker_Stability', 'PAE_Junction_Mean'\n",
        "]\n",
        "summary_columns = [c for c in summary_columns if c in combined_df.columns]\n",
        "summary_df = combined_df[summary_columns].copy()\n",
        "\n",
        "# Sort by Rigidity Score\n",
        "if 'Rigidity_Score' in summary_df.columns:\n",
        "    summary_df = summary_df.sort_values('Rigidity_Score', ascending=False)\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE TO EXCEL (MULTIPLE SHEETS)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 4/5] Saving results to Excel and CSV...\")\n",
        "\n",
        "excel_path = Path(DOWNLOAD_DIR) / \"BiTE_Analysis_Results.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    # Sheet 1: Summary (key metrics only)\n",
        "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "    print(f\"  ✓ Sheet 'Summary': {len(summary_df)} rows × {len(summary_df.columns)} cols\")\n",
        "\n",
        "    # Sheet 2: All Combined Data\n",
        "    combined_df.to_excel(writer, sheet_name='All_Data', index=False)\n",
        "    print(f\"  ✓ Sheet 'All_Data': {len(combined_df)} rows × {len(combined_df.columns)} cols\")\n",
        "\n",
        "    # Sheet 3: NMA Results (if available)\n",
        "    if 'nma' in available_dfs:\n",
        "        available_dfs['nma'].to_excel(writer, sheet_name='NMA_Analysis', index=False)\n",
        "        print(f\"  ✓ Sheet 'NMA_Analysis': {len(available_dfs['nma'])} rows\")\n",
        "\n",
        "    # Sheet 4: JSON Results (if available)\n",
        "    if 'json' in available_dfs:\n",
        "        available_dfs['json'].to_excel(writer, sheet_name='pLDDT_PAE_Analysis', index=False)\n",
        "        print(f\"  ✓ Sheet 'pLDDT_PAE_Analysis': {len(available_dfs['json'])} rows\")\n",
        "\n",
        "    # Sheet 5: RMSD Comparison (if available)\n",
        "    if 'comparison' in available_dfs:\n",
        "        available_dfs['comparison'].to_excel(writer, sheet_name='RMSD_Comparison', index=False)\n",
        "        print(f\"  ✓ Sheet 'RMSD_Comparison': {len(available_dfs['comparison'])} rows\")\n",
        "\n",
        "    # Sheet 6: Column Descriptions\n",
        "    descriptions = pd.DataFrame([\n",
        "        ['Jobname', 'Sample identifier'],\n",
        "        ['Linker_Length', 'Number of amino acids in linker region'],\n",
        "        ['Original_RMSD', 'RMSD of original structure vs reference (Å)'],\n",
        "        ['AlphaFold_RMSD', 'RMSD of AlphaFold prediction vs reference (Å)'],\n",
        "        ['RMSD_Improvement', 'Original_RMSD - AlphaFold_RMSD (positive = improved)'],\n",
        "        ['Rigidity_Score', 'Composite rigidity score (0-1, higher = more rigid)'],\n",
        "        ['Rigidity_Rank', 'Rank by rigidity (1 = most rigid)'],\n",
        "        ['Effective_Stiffness', 'GNM-based stiffness (higher = more rigid)'],\n",
        "        ['Inter_Domain_Fluctuation', 'COM distance fluctuation between scFv1/scFv2 (Å, lower = better)'],\n",
        "        ['MSF_Linker_Mean', 'Mean square fluctuation of linker (Ų, lower = more rigid)'],\n",
        "        ['Deformability_Linker_Mean', 'ANM deformability of linker (0-1, lower = more rigid)'],\n",
        "        ['pTM_Score', 'AlphaFold predicted TM-score'],\n",
        "        ['pLDDT_Global_Mean', 'Mean pLDDT across all residues'],\n",
        "        ['pLDDT_Linker_Mean', 'Mean pLDDT of linker region (higher = more confident)'],\n",
        "        ['pLDDT_Linker_Min', 'Minimum pLDDT in linker region'],\n",
        "        ['pLDDT_Junction_Mean', 'Mean pLDDT at linker-scFv junctions'],\n",
        "        ['PAE_Inter_Domain', 'PAE between scFv1 and scFv2 (Å, lower = better)'],\n",
        "        ['PAE_Linker_Stability', 'PAE of linker to scFv domains (Å, lower = better)'],\n",
        "        ['PAE_Junction_Mean', 'PAE at junction regions (Å, lower = better)'],\n",
        "    ], columns=['Column', 'Description'])\n",
        "    descriptions.to_excel(writer, sheet_name='Column_Descriptions', index=False)\n",
        "    print(f\"  ✓ Sheet 'Column_Descriptions': metric explanations\")\n",
        "\n",
        "print(f\"\\n  ✓ Excel saved: {excel_path}\")\n",
        "\n",
        "# Also save as CSV\n",
        "csv_path = Path(DOWNLOAD_DIR) / \"BiTE_Analysis_Summary.csv\"\n",
        "summary_df.to_csv(csv_path, index=False)\n",
        "print(f\"  ✓ CSV saved: {csv_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE ZIP FILE WITH ALL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[Step 5/5] Creating ZIP file with all results...\")\n",
        "\n",
        "zip_path = Path(DOWNLOAD_DIR) / \"BiTE_Analysis_Complete.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add Excel file\n",
        "    zipf.write(excel_path, \"BiTE_Analysis_Results.xlsx\")\n",
        "    print(f\"  ✓ Added: BiTE_Analysis_Results.xlsx\")\n",
        "\n",
        "    # Add CSV file\n",
        "    zipf.write(csv_path, \"BiTE_Analysis_Summary.csv\")\n",
        "    print(f\"  ✓ Added: BiTE_Analysis_Summary.csv\")\n",
        "\n",
        "    # Add PDB files\n",
        "    pdb_dir = Path(ALPHAFOLD_OUTPUT_DIR)\n",
        "    pdb_files = list(pdb_dir.glob(\"*.pdb\"))\n",
        "    if pdb_files:\n",
        "        for pdb_file in pdb_files:\n",
        "            zipf.write(pdb_file, f\"PDB_files/{pdb_file.name}\")\n",
        "        print(f\"  ✓ Added: {len(pdb_files)} PDB files → PDB_files/\")\n",
        "\n",
        "    # Add JSON files\n",
        "    json_files = list(pdb_dir.glob(\"*.json\"))\n",
        "    if json_files:\n",
        "        for json_file in json_files:\n",
        "            zipf.write(json_file, f\"JSON_files/{json_file.name}\")\n",
        "        print(f\"  ✓ Added: {len(json_files)} JSON files → JSON_files/\")\n",
        "\n",
        "    # Add visualization if exists\n",
        "    plot_path = pdb_dir / \"FINAL_summary_plots.png\"\n",
        "    if plot_path.exists():\n",
        "        zipf.write(plot_path, \"FINAL_summary_plots.png\")\n",
        "        print(f\"  ✓ Added: FINAL_summary_plots.png\")\n",
        "\n",
        "print(f\"\\n  ✓ ZIP created: {zip_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DISPLAY SUMMARY TABLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL SUMMARY TABLE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Display key metrics\n",
        "display_cols = ['Jobname', 'Linker_Length', 'Rigidity_Score', 'Rigidity_Rank',\n",
        "                'Effective_Stiffness', 'Inter_Domain_Fluctuation',\n",
        "                'pLDDT_Linker_Mean', 'PAE_Inter_Domain']\n",
        "display_cols = [c for c in display_cols if c in summary_df.columns]\n",
        "display_df = summary_df[display_cols].copy()\n",
        "\n",
        "# Format for display\n",
        "for col in display_df.columns:\n",
        "    if col in ['Rigidity_Score', 'Effective_Stiffness']:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
        "    elif col in ['Inter_Domain_Fluctuation']:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
        "    elif 'pLDDT' in col:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else \"N/A\")\n",
        "    elif 'PAE' in col:\n",
        "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"N/A\")\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# INTERPRETATION GUIDE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"INTERPRETATION GUIDE\")\n",
        "print(\"-\" * 70)\n",
        "print(\"• Rigidity_Score: Higher = more rigid linker (0-1 scale)\")\n",
        "print(\"• Effective_Stiffness: Higher = stiffer structure\")\n",
        "print(\"• Inter_Domain_Fluctuation: Lower = more stable domain separation (Å)\")\n",
        "print(\"• pLDDT_Linker_Mean: Higher = more confident prediction (>70 good, >90 excellent)\")\n",
        "print(\"• PAE_Inter_Domain: Lower = better domain orientation confidence (Å)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# ============================================================================\n",
        "# TOP CANDIDATES\n",
        "# ============================================================================\n",
        "\n",
        "if 'Rigidity_Score' in summary_df.columns:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TOP 5 MOST RIGID CANDIDATES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    top5 = summary_df.head(5)\n",
        "\n",
        "    for rank, (idx, row) in enumerate(top5.iterrows(), 1):\n",
        "        print(f\"\\n#{rank}: {row['Jobname']}\")\n",
        "        if 'Linker_Length' in row:\n",
        "            print(f\"    Linker Length: {row['Linker_Length']} aa\")\n",
        "        print(f\"    Rigidity Score: {row['Rigidity_Score']:.4f}\")\n",
        "        if 'Effective_Stiffness' in row and pd.notna(row['Effective_Stiffness']):\n",
        "            print(f\"    Effective Stiffness: {row['Effective_Stiffness']:.6f}\")\n",
        "        if 'Inter_Domain_Fluctuation' in row and pd.notna(row['Inter_Domain_Fluctuation']):\n",
        "            print(f\"    Inter-Domain Fluctuation: {row['Inter_Domain_Fluctuation']:.4f} Å\")\n",
        "        if 'pLDDT_Linker_Mean' in row and pd.notna(row['pLDDT_Linker_Mean']):\n",
        "            print(f\"    Linker pLDDT: {row['pLDDT_Linker_Mean']:.1f}\")\n",
        "        if 'PAE_Inter_Domain' in row and pd.notna(row['PAE_Inter_Domain']):\n",
        "            print(f\"    Inter-Domain PAE: {row['PAE_Inter_Domain']:.2f} Å\")\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"GENERATING VISUALIZATIONS\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Rigidity Score Ranking\n",
        "if 'Rigidity_Score' in combined_df.columns:\n",
        "    ax1 = axes[0, 0]\n",
        "    sorted_df = combined_df.sort_values('Rigidity_Score', ascending=True)\n",
        "    colors = plt.cm.RdYlGn(sorted_df['Rigidity_Score'] / sorted_df['Rigidity_Score'].max())\n",
        "    ax1.barh(range(len(sorted_df)), sorted_df['Rigidity_Score'], color=colors)\n",
        "    ax1.set_yticks(range(len(sorted_df)))\n",
        "    ax1.set_yticklabels(sorted_df['Jobname'], fontsize=8)\n",
        "    ax1.set_xlabel('Rigidity Score')\n",
        "    ax1.set_title('Rigidity Score Ranking\\n(Higher = More Rigid)')\n",
        "    ax1.axvline(x=sorted_df['Rigidity_Score'].mean(), color='red', linestyle='--', label='Mean')\n",
        "    ax1.legend()\n",
        "\n",
        "# Plot 2: Stiffness vs Inter-Domain Fluctuation\n",
        "if 'Effective_Stiffness' in combined_df.columns and 'Inter_Domain_Fluctuation' in combined_df.columns:\n",
        "    ax2 = axes[0, 1]\n",
        "    scatter = ax2.scatter(combined_df['Effective_Stiffness'],\n",
        "                         combined_df['Inter_Domain_Fluctuation'],\n",
        "                         c=combined_df.get('Rigidity_Score', 'blue'),\n",
        "                         cmap='RdYlGn', s=100, edgecolors='black')\n",
        "    ax2.set_xlabel('Effective Stiffness')\n",
        "    ax2.set_ylabel('Inter-Domain Fluctuation (Å)')\n",
        "    ax2.set_title('Stiffness vs Inter-Domain Fluctuation')\n",
        "    plt.colorbar(scatter, ax=ax2, label='Rigidity Score')\n",
        "    for idx, row in combined_df.iterrows():\n",
        "        ax2.annotate(row['Jobname'],\n",
        "                    (row['Effective_Stiffness'], row['Inter_Domain_Fluctuation']),\n",
        "                    fontsize=6, alpha=0.7)\n",
        "\n",
        "# Plot 3: pLDDT Distribution by Domain\n",
        "if 'pLDDT_Linker_Mean' in combined_df.columns:\n",
        "    ax3 = axes[1, 0]\n",
        "    plddt_data = []\n",
        "    labels = []\n",
        "    if 'pLDDT_scFv1_Mean' in combined_df.columns:\n",
        "        plddt_data.append(combined_df['pLDDT_scFv1_Mean'].dropna())\n",
        "        labels.append('scFv1')\n",
        "    if 'pLDDT_Linker_Mean' in combined_df.columns:\n",
        "        plddt_data.append(combined_df['pLDDT_Linker_Mean'].dropna())\n",
        "        labels.append('Linker')\n",
        "    if 'pLDDT_scFv2_Mean' in combined_df.columns:\n",
        "        plddt_data.append(combined_df['pLDDT_scFv2_Mean'].dropna())\n",
        "        labels.append('scFv2')\n",
        "    if plddt_data:\n",
        "        bp = ax3.boxplot(plddt_data, labels=labels, patch_artist=True)\n",
        "        colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "        for patch, color in zip(bp['boxes'], colors[:len(plddt_data)]):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_alpha(0.7)\n",
        "        ax3.set_ylabel('pLDDT Score')\n",
        "        ax3.set_title('pLDDT Distribution by Domain')\n",
        "        ax3.axhline(y=70, color='orange', linestyle='--', label='Confident (70)')\n",
        "        ax3.axhline(y=90, color='green', linestyle='--', label='Very High (90)')\n",
        "        ax3.legend(fontsize=8)\n",
        "\n",
        "# Plot 4: PAE Metrics\n",
        "if 'PAE_Inter_Domain' in combined_df.columns:\n",
        "    ax4 = axes[1, 1]\n",
        "    pae_cols = ['PAE_Inter_Domain', 'PAE_Linker_Stability', 'PAE_Linker_Internal']\n",
        "    pae_cols = [c for c in pae_cols if c in combined_df.columns]\n",
        "    if pae_cols:\n",
        "        x = np.arange(len(combined_df))\n",
        "        width = 0.25\n",
        "        for i, col in enumerate(pae_cols):\n",
        "            ax4.bar(x + i*width, combined_df[col], width, label=col.replace('PAE_', ''))\n",
        "        ax4.set_xlabel('Sample')\n",
        "        ax4.set_ylabel('PAE (Å)')\n",
        "        ax4.set_title('PAE Metrics by Sample')\n",
        "        ax4.set_xticks(x + width)\n",
        "        ax4.set_xticklabels(combined_df['Jobname'], rotation=45, ha='right', fontsize=8)\n",
        "        ax4.legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = Path(DOWNLOAD_DIR) / \"FINAL_summary_plots.png\"\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"  ✓ Saved: {plot_path}\")\n",
        "\n",
        "# Add plot to ZIP\n",
        "with zipfile.ZipFile(zip_path, 'a') as zipf:\n",
        "    zipf.write(plot_path, \"FINAL_summary_plots.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# DOWNLOAD LINKS (for Colab)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOWNLOAD FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(f\"\\nFiles ready for download in: {DOWNLOAD_DIR}\")\n",
        "print(f\"\\n📁 Available files:\")\n",
        "print(f\"  • BiTE_Analysis_Results.xlsx - Excel with all results (6 sheets)\")\n",
        "print(f\"  • BiTE_Analysis_Summary.csv - Key metrics summary\")\n",
        "print(f\"  • BiTE_Analysis_Complete.zip - All files including PDB & JSON\")\n",
        "print(f\"  • FINAL_summary_plots.png - Visualization\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"Click the links below to download:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Download Excel\n",
        "    print(\"\\n📊 Downloading Excel file...\")\n",
        "    files.download(str(excel_path))\n",
        "\n",
        "    # Download ZIP\n",
        "    print(\"\\n📦 Downloading ZIP file (contains all PDB, JSON, CSV)...\")\n",
        "    files.download(str(zip_path))\n",
        "else:\n",
        "    print(f\"\\n📂 Files are saved in: {DOWNLOAD_DIR}\")\n",
        "    print(\"   You can download them from the file browser on the left.\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL OUTPUT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "Output Summary:\n",
        "  • Total samples analyzed: {len(combined_df)}\n",
        "  • Excel file: BiTE_Analysis_Results.xlsx (6 sheets)\n",
        "  • ZIP file: BiTE_Analysis_Complete.zip\n",
        "    - PDB files: {len(list(Path(ALPHAFOLD_OUTPUT_DIR).glob('*.pdb')))}\n",
        "    - JSON files: {len(list(Path(ALPHAFOLD_OUTPUT_DIR).glob('*.json')))}\n",
        "    - Summary CSV and Excel\n",
        "    - Visualization plots\n",
        "\"\"\")\n",
        "\n",
        "# Store for further analysis\n",
        "final_results_df = combined_df.copy()\n",
        "final_summary_df = summary_df.copy()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Variables available for further analysis:\")\n",
        "print(\"  • final_results_df - All combined data\")\n",
        "print(\"  • final_summary_df - Key metrics summary\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ]
}